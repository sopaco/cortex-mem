# System Boundary Interface Documentation

This document describes the system's external invocation interfaces, including CLI commands, API endpoints, configuration parameters, and other boundary mechanisms.

## Command Line Interface (CLI)

### cortex-mem-cli

**Description**: CLI tool for managing memories and sessions

**Source File**: `cortex-mem-cli/src/main.rs`

**Arguments**:

- `config` (PathBuf): required - Path to configuration file
- `tenant` (String): required - Tenant identifier for multi-tenancy
- `verbose` (bool): optional - Enable verbose logging

**Options**:

- `--config, -c`(PathBuf): required - Configuration file path
- `--tenant, -t`(String): required - Tenant ID
- `--verbose, -v`(bool): optional - Enable verbose output (default: `false`)

**Usage Examples**:

```bash
cortex-mem-cli --config /etc/cortex/config.toml --tenant acme add --thread thread-123 --role user --content "Hello"
```

```bash
cortex-mem-cli -c config.toml -t acme search --query "machine learning" --limit 10
```

```bash
cortex-mem-cli --config config.toml --tenant acme session create --thread thread-456 --title "My Session"
```

```bash
cortex-mem-cli --config config.toml --tenant acme list --uri /memories --include-abstracts
```

```bash
cortex-mem-cli --config config.toml --tenant acme stats
```

### cortex-mem-mcp

**Description**: MCP (Model Context Protocol) server for memory operations via stdio

**Source File**: `cortex-mem-mcp/src/main.rs`

**Arguments**:

- `config` (PathBuf): required - Path to configuration file
- `tenant` (String): required - Tenant identifier

**Options**:

- `--config, -c`(PathBuf): required - Configuration file path
- `--tenant, -t`(String): required - Tenant ID

**Usage Examples**:

```bash
cortex-mem-mcp --config /etc/cortex/config.toml --tenant acme
```

### cortex-mem-service

**Description**: HTTP service for memory operations with RESTful API

**Source File**: `cortex-mem-service/src/main.rs`

**Arguments**:

- `data_dir` (String): required - Data directory for persistence
- `host` (String): required - Server host address
- `port` (u16): required - Server port
- `verbose` (bool): optional - Enable verbose logging

**Options**:

- `--data-dir, -d`(String): required - Data directory path
- `--host, -h`(String): required - Host address to bind
- `--port, -p`(u16): required - Port to listen on
- `--verbose, -v`(bool): optional - Enable verbose logging (default: `false`)

**Usage Examples**:

```bash
cortex-mem-service --data-dir /var/lib/cortex --host 0.0.0.0 --port 8080
```

```bash
cortex-mem-service -d /data -h 127.0.0.1 -p 3000 --verbose
```

### cortex-mem-tars

**Description**: TARS (Talk, Ask, Retrieve, Store) example application with memory integration

**Source File**: `examples/cortex-mem-tars/src/main.rs`

**Arguments**:

- `enhance_memory_saver` (bool): optional - Enable memory saver enhancement
- `enable_audio_connect` (bool): optional - Enable audio connect feature
- `audio_connect_mode` (String): optional - Audio connect mode setting
- `enhance_vector_search` (bool): optional - Enable vector search enhancement

**Options**:

- `--enhance-memory-saver`(bool): optional - Enhance memory saver functionality (default: `false`)
- `--enable-audio-connect`(bool): optional - Enable audio connect integration (default: `false`)
- `--audio-connect-mode`(String): optional - Audio connect mode (stdio, websocket, sse) (default: `stdio`)
- `--enhance-vector-search`(bool): optional - Enhance vector search capabilities (default: `false`)

**Usage Examples**:

```bash
cortex-mem-tars --enhance-memory-saver --enable-audio-connect --audio-connect-mode websocket
```

```bash
cortex-mem-tars --enhance-vector-search
```

## API Interfaces

### GET GET /api/memory/

**Description**: Memory Management API for creating, retrieving, updating, and deleting memories with vector search capabilities

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: Query parameters: user_id?, agent_id?, run_id?, actor_id?, memory_type?, limit?, offset?

**Response Format**: JSON: { total: number, memories: Array<{id, content, metadata, created_at, updated_at}> }

### POST POST /api/memory/search

**Description**: Search memories using semantic similarity with vector embedding

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: JSON: { query: string, user_id?, agent_id?, run_id?, actor_id?, memory_type?, limit?, similarity_threshold? }

**Response Format**: JSON: { total: number, results: Array<{memory: MemoryResponse, score: number}> }

### GET GET /api/memory/:id

**Description**: Retrieve a specific memory by ID

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: Path parameter: id

**Response Format**: JSON: MemoryResponse { id, content, metadata, created_at, updated_at }

### POST POST /api/memory/

**Description**: Create a new memory entry

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: JSON: { content: string, metadata?: object }

**Response Format**: JSON: MemoryResponse

### PUT PUT /api/memory/:id

**Description**: Update an existing memory

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: JSON: { content: string }

**Response Format**: JSON: MemoryResponse

### DELETE DELETE /api/memory/:id

**Description**: Delete a memory by ID

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: Path parameter: id

**Response Format**: JSON: { success: boolean }

### POST POST /api/memory/batch/delete

**Description**: Batch delete multiple memories

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: JSON: { ids: string[] }

**Response Format**: JSON: { deleted: number }

### GET GET /api/memory/stats/summary

**Description**: Get memory statistics summary

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: None

**Response Format**: JSON: Statistics object

### GET GET /api/memory/stats/types

**Description**: Get memory statistics by types

**Source File**: `cortex-mem-insights/src/server/api/memory.ts`

**Request Format**: None

**Response Format**: JSON: Type distribution statistics

### POST POST /api/optimization/

**Description**: Trigger memory optimization process

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: JSON: { memory_type?, user_id?, agent_id?, run_id?, actor_id?, similarity_threshold?, dry_run?, verbose?, strategy?, aggressive?, timeout_minutes? }

**Response Format**: JSON: OptimizationJob { jobId, status }

### GET GET /api/optimization/history

**Description**: Get optimization history

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: Query parameters

**Response Format**: JSON: Array<OptimizationRecord>

### GET GET /api/optimization/statistics

**Description**: Get optimization statistics

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: None

**Response Format**: JSON: OptimizationStatistics

### POST POST /api/optimization/analyze

**Description**: Analyze optimization opportunities

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: JSON: Analysis parameters

**Response Format**: JSON: AnalysisResult

### GET GET /api/optimization/:jobId

**Description**: Get optimization job status by ID

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: Path parameter: jobId

**Response Format**: JSON: OptimizationJob

### POST POST /api/optimization/:jobId/cancel

**Description**: Cancel an optimization job

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: Path parameter: jobId

**Response Format**: JSON: { success: boolean }

### POST POST /api/optimization/cleanup

**Description**: Cleanup old optimization history

**Source File**: `cortex-mem-insights/src/server/api/optimization.ts`

**Request Format**: JSON: { maxAgeDays?: number }

**Response Format**: JSON: { cleaned: number }

### GET GET /api/system/status

**Description**: Get system status overview

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: SystemStatus { status, version, uptime, components }

### GET GET /api/system/vector-store/status

**Description**: Get vector store connection status

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: { connected: boolean, latency: number, collections: number }

### GET GET /api/system/llm/status

**Description**: Get LLM service status

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: { available: boolean, model: string, queue_size: number }

### GET GET /api/system/metrics

**Description**: Get system performance metrics

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: PerformanceMetrics { cpu, memory, requests, latency }

### GET GET /api/system/info

**Description**: Get system information

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: SystemInfo { version, platform, config }

### GET GET /api/system/logs

**Description**: Get system logs

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: Query: level?, limit?, since?

**Response Format**: JSON: Array<LogEntry>

### GET GET /api/system/resources

**Description**: Get system resource usage

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: { cpu, memory, disk, network }

### POST POST /api/system/clear-cache

**Description**: Clear system cache

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: None

**Response Format**: JSON: { cleared: boolean, bytes: number }

### POST POST /api/system/restart

**Description**: Restart system services

**Source File**: `cortex-mem-insights/src/server/api/system.ts`

**Request Format**: JSON: { force?: boolean }

**Response Format**: JSON: { success: boolean, message: string }

### GET GET /health

**Description**: Health check endpoint

**Source File**: `cortex-mem-insights/src/server/index.ts`

**Request Format**: None

**Response Format**: JSON: { status: string, timestamp: string }

### POST POST /api/memory/store

**Description**: Store memory via TARS API

**Source File**: `examples/cortex-mem-tars/src/api_server.rs`

**Request Format**: JSON: StoreMemoryRequest

**Response Format**: JSON: StoreMemoryResponse

### GET GET /api/memory/retrieve

**Description**: Retrieve memories via TARS API

**Source File**: `examples/cortex-mem-tars/src/api_server.rs`

**Request Format**: Query: { query?, speaker_type?, limit? }

**Response Format**: JSON: RetrieveMemoryResponse

### GET GET /api/memory/list

**Description**: List memories via TARS API

**Source File**: `examples/cortex-mem-tars/src/api_server.rs`

**Request Format**: Query: { speaker_type?, limit?, offset? }

**Response Format**: JSON: ListMemoryResponse

### GET GET /api/memory/health

**Description**: Health check for memory service

**Source File**: `examples/cortex-mem-tars/src/api_server.rs`

**Request Format**: None

**Response Format**: JSON: { status: string }

### GET GET /filesystem/list

**Description**: List directory contents

**Source File**: `cortex-mem-service/src/handlers/filesystem.rs`

**Request Format**: Query: { uri: string }

**Response Format**: JSON: ApiResponse<Array<FileEntryResponse>>

### GET GET /filesystem/read/:path

**Description**: Read file content

**Source File**: `cortex-mem-service/src/handlers/filesystem.rs`

**Request Format**: Path parameter: path (URL encoded)

**Response Format**: JSON: ApiResponse<string>

### GET GET /sessions/

**Description**: List all sessions

**Source File**: `cortex-mem-service/src/handlers/sessions.rs`

**Request Format**: None

**Response Format**: JSON: ApiResponse<Array<SessionResponse>>

### POST POST /sessions/

**Description**: Create new session

**Source File**: `cortex-mem-service/src/handlers/sessions.rs`

**Request Format**: JSON: { thread_id?, title?, metadata? }

**Response Format**: JSON: ApiResponse<SessionResponse>

### POST POST /sessions/:thread_id/messages

**Description**: Add message to session

**Source File**: `cortex-mem-service/src/handlers/sessions.rs`

**Request Format**: JSON: { role: string, content: string, metadata? }

**Response Format**: JSON: ApiResponse<string>

### POST POST /sessions/:thread_id/close

**Description**: Close session

**Source File**: `cortex-mem-service/src/handlers/sessions.rs`

**Request Format**: Path parameter: thread_id

**Response Format**: JSON: ApiResponse<SessionResponse>

### POST POST /search/

**Description**: Search memories with semantic similarity

**Source File**: `cortex-mem-service/src/handlers/search.rs`

**Request Format**: JSON: { query: string, thread?, limit?, min_score?, scope? }

**Response Format**: JSON: ApiResponse<Array<SearchResultResponse>>

### POST POST /automation/extract/:thread_id

**Description**: Trigger extraction for thread

**Source File**: `cortex-mem-service/src/handlers/automation.rs`

**Request Format**: JSON: { auto_save?: boolean }

**Response Format**: JSON: ApiResponse<ExtractionResult>

### POST POST /automation/index/:thread_id

**Description**: Trigger indexing for thread

**Source File**: `cortex-mem-service/src/handlers/automation.rs`

**Request Format**: Path parameter: thread_id

**Response Format**: JSON: ApiResponse<IndexResult>

### POST POST /automation/index-all

**Description**: Trigger indexing for all threads

**Source File**: `cortex-mem-service/src/handlers/automation.rs`

**Request Format**: None

**Response Format**: JSON: ApiResponse<IndexAllResult>

### POST POST /embed (external)

**Description**: Embedding service for text vectorization

**Source File**: `cortex-mem-core/src/embedding/client.rs`

**Request Format**: JSON: { text: string } or { texts: string[] }

**Response Format**: JSON: { embedding: number[] } or { embeddings: number[][] }

### CLI Server startup

**Description**: Start API server with custom configuration

**Source File**: `cortex-mem-service/src/main.rs`

**Request Format**: CLI args: data_dir, host, port, verbose

**Response Format**: Running server

## Router Routes

### /monitor

**Description**: SvelteKit page for monitoring system metrics, performance, and health status

**Source File**: `cortex-mem-insights/src/routes/monitor/+page.svelte`

**Parameters**:

- `currentPath` (string): Current URL pathname for navigation highlighting
### /

**Description**: SvelteKit root layout with navigation and i18n support

**Source File**: `cortex-mem-insights/src/routes/+layout.svelte`

**Parameters**:

- `currentPath` (string): Current page URL pathname from $page store
### /

**Description**: SvelteKit layout configuration for client-side rendering

**Source File**: `cortex-mem-insights/src/routes/+layout.ts`

### /health

**Description**: Health check endpoint for service availability

**Source File**: `cortex-mem-insights/src/server/index.ts`

### /api/memory/*

**Description**: API routes for memory management operations

**Source File**: `cortex-mem-insights/src/server/index.ts`

**Parameters**:

- `*` (string): Memory operation endpoint path
### /api/optimization/*

**Description**: API routes for memory optimization operations

**Source File**: `cortex-mem-insights/src/server/index.ts`

**Parameters**:

- `*` (string): Optimization operation endpoint path
### /api/system/*

**Description**: API routes for system management

**Source File**: `cortex-mem-insights/src/server/index.ts`

**Parameters**:

- `*` (string): System operation endpoint path
### /filesystem/:action/:path?

**Description**: Filesystem routes for directory and file operations

**Source File**: `cortex-mem-service/src/routes/filesystem.rs`

**Parameters**:

- `action` (string): Operation type (list or read)
- `path` (string): File or directory path
### /automation/:action/:thread_id?

**Description**: Automation routes for extraction and indexing

**Source File**: `cortex-mem-service/src/routes/automation.rs`

**Parameters**:

- `action` (string): Operation type (extract, index, or index-all)
- `thread_id` (string): Thread ID for extraction/indexing
### /sessions/:thread_id/:subaction?

**Description**: Session management routes

**Source File**: `cortex-mem-service/src/routes/sessions.rs`

**Parameters**:

- `thread_id` (string): Thread ID for specific session
- `subaction` (string): Sub-action (messages, close)
### /search/

**Description**: Search endpoint for semantic memory queries

**Source File**: `cortex-mem-service/src/routes/search.rs`

### /api/:module/*

**Description**: API router combining all module routes

**Source File**: `cortex-mem-service/src/routes/mod.rs`

**Parameters**:

- `module` (string): Module path (filesystem, sessions, search, automation)
## Integration Suggestions

### Web Dashboard Integration

Integration with Cortex Memory Insights Web Dashboard

**Example Code**:

```
import { api } from '@cortex-mem/insights-client';

// Test connection
const connectionTest = await api.testConnection('http://localhost:8000');
console.log('Connection status:', connectionTest);

// Search memories
const results = await api.memory.search({
  query: 'project requirements',
  memory_type: 'conversation',
  limit: 10,
  similarity_threshold: 0.7
});

// Create memory
const memory = await api.memory.create({
  content: 'Important project decision: Use Rust for backend',
  metadata: { project: 'cortex-mem', priority: 'high' }
});

// Optimize memories
const optimizationJob = await api.optimization.optimize({
  strategy: 'deduplication',
  similarity_threshold: 0.95,
  dry_run: false
});

// Monitor optimization
const status = await api.optimization.getStatus(optimizationJob.jobId);

// System monitoring
const metrics = await api.system.metrics();
const systemStatus = await api.system.status();
```

**Best Practices**:

- Always configure CORS origins explicitly for production deployments
- Use connection pooling for LLM and vector store clients
- Implement proper error handling with onError hooks
- Enable tracing for request/response logging
- Use environment variables for sensitive configuration (API keys)
- Implement retry logic for embedding service calls
- Use batch operations for bulk memory operations
- Monitor memory usage and implement cleanup jobs
- Use similarity thresholds to filter low-quality search results
- Implement graceful shutdown handlers

### MCP Protocol Integration

Model Context Protocol (MCP) integration for AI assistants

**Example Code**:

```
// Initialize MCP service
use cortex_mem_mcp::MemoryMcpService;
use cortex_mem_core::{Config, LLMClientImpl, MemoryOperations};

#[tokio::main]
async fn main() -> Result<()> {
    let config = Config::load("/etc/cortex/config.toml")?;
    let _ = tracing_subscriber::fmt::init();
    
    let llm_client = LLMClientImpl::new(&config.llm).await?;
    let memory_ops = MemoryOperations::new(&config, &llm_client).await?;
    
    let mcp_service = MemoryMcpService::new(memory_ops);
    mcp_service.serve_stdio().await?;
    
    Ok(())
}
```

**Best Practices**:

- Initialize LLM client before MemoryOperations
- Store configuration in TOML format
- Use tenant isolation for multi-tenant scenarios
- Implement proper error handling for MCP operations
- Enable tracing for debugging
- Use stdio transport for MCP protocol compliance

### Rust Service Router Integration

Rust Axum service integration with modular routes

**Example Code**:

```
use axum::{Router, routing::{get, post}};
use std::sync::Arc;

// Build modular router
let api_routes = Router::new()
    .nest("/filesystem", filesystem::routes())
    .nest("/sessions", sessions::routes())
    .nest("/search", search::routes())
    .nest("/automation", automation::routes());

let app = Router::new()
    .route("/health", get(health_check))
    .nest("/api", api_routes)
    .with_state(Arc::new(app_state));

// Layered search implementation
async fn search_layered(
    state: &AppState,
    query: &str,
    thread: Option<&str>,
    limit: usize,
    min_score: f32
) -> Result<Vec<SearchResultResponse>> {
    // First pass: semantic search
    let results = state.vector_store.search(
        &embedding,
        limit * 2,
        min_score
    ).await?;
    
    // Second pass: rerank by relevance
    let reranked = rerank_results(results, query).await?;
    
    Ok(reranked.into_iter().take(limit).collect())
}
```

**Best Practices**:

- Use the service router for modular API design
- Implement proper state management with Arc<AppState>
- Use layered search for complex queries
- Implement health checks for service monitoring
- Use JSON API responses consistently
- Handle file path encoding properly for filesystem routes
- Use batch indexing for large datasets

### Bun/Elysia TypeScript Server

Bun/Elysia TypeScript server for Insights API

**Example Code**:

```
import { Elysia } from 'elysia';
import { cors } from '@elysiajs/cors';
import { memoryRoutes } from './api/memory';
import { optimizationRoutes } from './api/optimization';
import { systemRoutes } from './api/system';

const app = new Elysia()
  .use(cors({
    origin: ['http://localhost:5173', 'http://localhost:8000'],
    credentials: true
  }))
  .onError(({ code, error, set }) => {
    console.error(`[${code}]`, error);
    set.status = 500;
    return { error: error.message, code };
  })
  .get('/health', () => ({ status: 'ok', timestamp: new Date().toISOString() }))
  .group('/api/memory', (app) => app.use(memoryRoutes))
  .group('/api/optimization', (app) => app.use(optimizationRoutes))
  .group('/api/system', (app) => app.use(systemRoutes))
  .listen(3000);

console.log(`Server running at ${app.server?.hostname}:${app.server?.port}`);
```

**Best Practices**:

- Use Elysia for high-performance Bun-based servers
- Implement CORS for frontend integration
- Use structured logging with onError hooks
- Implement graceful error responses
- Use TypeScript for type safety
- Organize routes by domain (memory, optimization, system)

### TARS Application Template

TARS (Talk, Ask, Retrieve, Store) application integration pattern

**Example Code**:

```
use cortex_mem_tars::{ConfigManager, Infrastructure, App};

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    
    // Configuration
    let config_manager = ConfigManager::new()?;
    init_logger();
    
    // Create bots
    let bots = create_default_bots(&config_manager);
    
    // Infrastructure
    let infrastructure = Infrastructure::new(
        &config_manager,
        args.enhance_memory_saver,
        args.enable_audio_connect,
        &args.audio_connect_mode,
        args.enhance_vector_search
    ).await?;
    
    // Application
    let app = App::new(bots, infrastructure)?;
    
    // Check services
    app.check_service_status().await?;
    
    // Run and cleanup
    app.run().await?;
    
    // Conditional save on exit
    if args.enhance_memory_saver {
        app.on_exit().await?;
    }
    
    Ok(())
}
```

**Best Practices**:

- Initialize ConfigManager before other components
- Use conditional memory saving on exit
- Start API server for external integrations
- Check service status before operations
- Use proper logging initialization
- Implement graceful shutdown with on_exit handlers

### Automation Indexing Integration

Automated indexing for conversation threads and messages

**Example Code**:

```
use cortex_mem_core::automation::{AutoIndexer, IndexerConfig};

// Configuration
let config = IndexerConfig {
    auto_index: true,
    batch_size: 100,
    async_index: true,
};

// Initialize indexer
let indexer = AutoIndexer::new(
    Arc::new(filesystem),
    Arc::new(embedding_client),
    Arc::new(vector_store),
    config
);

// Index thread with progress
indexer.index_thread_with_progress(
    &thread_id,
    |progress| println!("Indexed: {:.1}%", progress * 100.0)
).await?;

// Collect messages recursively
let messages = indexer.collect_messages_recursive(&thread_uri).await?;

// Batch index
indexer.index_message_batch(&messages).await?;
```

**Best Practices**:

- Use AutoIndexer for batch processing
- Enable async indexing for performance
- Calculate content hash to avoid duplicate indexing
- Use progress callbacks for long-running operations
- Parse markdown content for structured extraction
- Configure batch size based on embedding service capacity


---

**Analysis Confidence**: 9.0/10
