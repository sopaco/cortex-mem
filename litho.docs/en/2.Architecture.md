 # System Architecture Documentation

**Cortex-Mem: Multi-Dimensional Memory System for AI Agents**

**Version**: 1.0  
**Generation Time**: 2026-02-19 04:01:33 (UTC)  
**Classification**: Architecture Overview (C4 Model - Container Level)

---

## 1. Architecture Overview

### 1.1 Design Philosophy

Cortex-Mem implements a **layered, domain-driven architecture** designed to provide persistent, context-aware memory capabilities for AI agent systems. The architecture adheres to the following core principles:

- **Separation of Concerns**: Clear boundaries between core business logic (`cortex-mem-core`) and application interfaces (CLI, HTTP, MCP)
- **Event-Driven Automation**: Asynchronous, decoupled processing pipelines for indexing and extraction tasks
- **Multi-Tenant Isolation**：Infrastructure-level scoping ensures data segregation across tenants via collection suffixing and directory partitioning
- **Hybrid Persistence**: Dual-storage strategy combining filesystem-based source of truth (markdown) with vector-based semantic search (Qdrant)
- **Interface Diversity**: Multiple access patterns supporting different integration scenarios—from direct library usage to Model Context Protocol (MCP) servers

### 1.2 Core Architecture Patterns

| Pattern | Implementation | Purpose |
|---------|---------------|---------|
| **Workspace Pattern** | Rust workspace with 8+ specialized crates | Modular deployment and selective dependency management |
| **Hexagonal Architecture** | Core domain with adapter layers for storage and interfaces | Technology-agnostic business logic |
| **Event Sourcing** | Async event bus (tokio mpsc) for automation triggers | Decoupled background processing |
| **Three-Tier Memory Hierarchy** | L0 (Abstract) → L1 (Overview) → L2 (Detail) | Optimized retrieval with progressive precision |
| **URI Resource Addressing** | Custom `cortex://` scheme | Location transparency and portable resource references |

### 1.3 Technology Stack Overview

```mermaid
graph TB
    subgraph CoreStack["Core Technology Stack"]
        direction TB
        RUST[Rust 1.75+]
        TOKIO[Tokio Async Runtime]
        AXUM[Axum Web Framework]
        SERDE[Serde Serialization]
    end
    
    subgraph Storage["Storage Layer"]
        QDRANT[Qdrant Vector DB]
        FS[OS Filesystem]
    end
    
    subgraph AI["AI/ML Integration"]
        LLM[OpenAI-compatible APIs]
        EMB[Embedding Models]
    end
    
    subgraph Frontend["Frontend"]
        SVELTE[Svelte 5]
        TYPESCRIPT[TypeScript]
    end
    
    RUST --> TOKIO
    TOKIO --> AXUM
    RUST --> SERDE
    RUST --> QDRANT
    RUST --> FS
    RUST --> LLM
    RUST --> EMB
    SVELTE --> TYPESCRIPT
```

**Key Technical Decisions**:
- **Rust**: Type safety, performance, and async ecosystem for I/O-bound operations
- **Qdrant**: Native Rust client support, tenant-aware collections, metadata filtering
- **Axum**: Tower ecosystem compatibility for middleware and observability
- **Svelte 5**: Reactive frontend for real-time memory visualization with minimal bundle size

---

## 2. System Context

### 2.1 System Positioning and Value Proposition

Cortex-Mem serves as **infrastructure middleware** for AI agent systems, solving the "goldfish memory" problem where agents lose context between interactions. It provides:

- **Persistent Context**: Cross-session memory retention with hierarchical organization
- **Semantic Retrieval**: Vector-based search enabling conceptual rather than keyword matching
- **Automated Knowledge Extraction**: LLM-powered profiling of users and agents from conversation histories
- **Multi-Dimensional Storage**: Isolated memory spaces for users, agents, sessions, and resources

### 2.2 User Roles and Scenarios

```mermaid
graph LR
    subgraph Users["User Ecosystem"]
        DEV[AI Agent Developers]
        END[End Users]
        OPS[System Administrators]
    end
    
    subgraph Value["Value Delivery"]
        LIB[Library APIs<br/>Rust Integration]
        CLI[CLI Tool<br/>Direct Operations]
        HTTP[REST API<br/>Service Integration]
        MCP[MCP Server<br/>AI Assistant Integration]
        UI[Insights UI<br/>Visualization & Monitoring]
    end
    
    DEV --> LIB
    DEV --> CLI
    DEV --> HTTP
    OPS --> CLI
    OPS --> HTTP
    OPS --> UI
    END -.->|Benefits from| MCP
    END -.->|Benefits from| UI
    
    style DEV fill:#f96,stroke:#333
    style END fill:#9f9,stroke:#333
    style OPS fill:#99f,stroke:#333
```

**Primary User Scenarios**:

1. **AI Agent Developers**: Integrate `cortex-mem-core` library or HTTP APIs to add memory capabilities to agent frameworks
2. **System Administrators**: Deploy multi-tenant instances with per-tenant isolation, monitor vector storage usage via Insights UI
3. **End Users**: Experience personalized AI interactions where agents recall preferences, past decisions, and established facts

### 2.3 External System Interactions

```mermaid
graph TB
    subgraph CortexMem["Cortex-Mem System Boundary"]
        CORE[cortex-mem-core<br/>Business Logic]
        INTERFACES[Application Interfaces<br/>CLI / HTTP / MCP / Web]
    end
    
    subgraph External["External Systems"]
        LLM[(LLM API Provider<br/>OpenAI/Azure/Local)]
        QDRANT[(Qdrant Vector DB<br/>Port 6334)]
        OS_FS[(OS Filesystem<br/>Markdown Storage)]
    end
    
    subgraph Clients["Client Applications"]
        MCP_CLIENTS[MCP Clients<br/>AI Assistants / IDEs]
        BROWSERS[Web Browsers<br/>Insights Dashboard]
        API_CLIENTS[HTTP Clients<br/>External Services]
        TERMINAL[Terminal Users<br/>CLI Operations]
    end
    
    %% Client interactions
    TERMINAL -->|Command Line| INTERFACES
    API_CLIENTS -->|REST /api/v2| INTERFACES
    MCP_CLIENTS -->|stdio / MCP Protocol| INTERFACES
    BROWSERS -->|HTTP| INTERFACES
    
    %% Internal flow
    INTERFACES --> CORE
    
    %% External deps
    CORE -->|Embeddings &<br/>Completions| LLM
    CORE -->|Vector Search<br/>CRUD| QDRANT
    CORE -->|File I/O<br/>cortex:// URIs| OS_FS
    
    style CORE fill:#f9f,stroke:#333,stroke-width:4px
    style QDRANT fill:#e1f5ff,stroke:#01579b
    style LLM fill:#e1f5ff,stroke:#01579b
```

**Boundary Definition**:
- **Included**: All workspace crates, local filesystem storage, local/remote LLM client integration, MCP protocol implementation
- **Excluded**: External LLM infrastructure (OpenAI/Anthropic APIs), external Qdrant clusters, client applications, authentication providers

---

## 3. Container View

### 3.1 Workspace Architecture

Cortex-Mem follows a **Rust workspace pattern** with strict dependency directionality toward the core library. The architecture organizes components into four logical layers:

```mermaid
graph TD
    subgraph Workspace["Cortex-Mem Workspace"]
        direction TB
        
        subgraph InterfaceLayer["Interface Layer"]
            CLI[cortex-mem-cli<br/>Command Line Tool]
            SERVICE[cortex-mem-service<br/>REST API Server<br/>Axum-based]
            MCP[cortex-mem-mcp<br/>MCP Protocol Server]
            INSIGHTS[cortex-mem-insights<br/>Svelte Web Dashboard]
        end
        
        subgraph ApplicationLayer["Application Layer"]
            TOOLS[cortex-mem-tools<br/>Tool Definitions & Operations]
            RIG[cortex-mem-rig<br/>Rig Framework Adapter]
        end
        
        subgraph DomainLayer["Domain Layer"]
            CORE[cortex-mem-core<br/>Core Business Logic]
        end
        
        subgraph InfrastructureLayer["Infrastructure Layer"]
            CONFIG[cortex-mem-config<br/>Configuration Management]
        end
    end
    
    %% Dependency directions - strict flow toward core
    CLI --> TOOLS
    CLI --> CORE
    CLI --> CONFIG
    
    SERVICE --> CORE
    SERVICE --> CONFIG
    
    MCP --> CORE
    MCP --> TOOLS
    
    INSIGHTS -.->|HTTP API Calls| SERVICE
    
    TOOLS --> CORE
    
    RIG --> TOOLS
    RIG --> CORE
    
    CORE --> CONFIG
    
    style CORE fill:#f96,stroke:#333,stroke-width:3px
    style CONFIG fill:#9f9,stroke:#333
    style CLI fill:#99f,stroke:#333
    style SERVICE fill:#99f,stroke:#333
    style MCP fill:#99f,stroke:#333
```

### 3.2 Domain Module Division

The system comprises **12 distinct domains** organized by technical responsibility and business capability:

| Domain | Type | Responsibility | Key Components |
|--------|------|----------------|----------------|
| **Configuration Management** | Infrastructure | TOML/env configuration, tenant isolation setup | Config Loader, Multi-Tenant Config, Builder |
| **Core Infrastructure** | Core Business | Filesystem abstraction, LLM clients, vector storage, event bus | CortexFilesystem, LLMClient, QdrantVectorStore |
| **Automation Management** | Core Business | Background processing, file watching, indexing | AutomationManager, AutoIndexer, AutoExtractor, SyncManager |
| **Layer Management** | Core Business | Three-tier hierarchy generation (L0/L1/L2) | LayerManager, AbstractGenerator, OverviewGenerator |
| **Extraction Engine** | Core Business | LLM-powered knowledge extraction from conversations | MemoryExtractor, Extraction Types |
| **Search Engine** | Core Business | Semantic search with weighted scoring | VectorEngine, Intent Detection |
| **Session Management** | Core Business | Conversation state, message tracking, timelines | SessionManager, Timeline, Message |
| **Profile Management** | Core Business | User/agent profile persistence and merging | UserProfile, AgentProfile |
| **Application Interface** | Application | Multi-protocol access layer (CLI, HTTP, MCP) | CLI Commands, Axum Handlers, MCP Service |
| **Web UI** | Application | Visualization and management interface | Svelte Pages, Tenant Store, API Client |
| **Tool Support** | Application | MCP tool definitions, Rig framework integration | Tool Definitions, Operation Wrappers |
| **Vector Storage** | Infrastructure | Qdrant client with tenant-aware collections | QdrantVectorStore, URI-to-ID mapping |

### 3.3 Storage Design

The system implements a **hybrid storage architecture** combining filesystem durability with vector search performance:

```mermaid
graph TB
    subgraph Storage["Hybrid Storage Architecture"]
        direction TB
        
        subgraph FS["Filesystem Layer (Source of Truth)"]
            MD["Markdown Files\n.md"]
            STRUCT["Structured JSON\nProfiles and Metadata"]
            URI_SCHEME["cortex:// URI Scheme"]
            
            subgraph Layout["Tenant Scoped Directory Layout"]
                TENANT["data/tenants/{tenant_id}/"]
                SESSION["session/{id}/timeline/"]
                USER["user/{id}/profile.json"]
                AGENT["agent/{id}/profile.json"]
                RESOURCES["resources/{type}/"]
            end
        end
        
        subgraph Vector["Vector Layer (Search Index)"]
            QDRANT["Qdrant Collections"]
            EMBEDDINGS["Vector Embeddings"]
            METADATA["Point Metadata\ntenant_id, uri, layer"]
            
            subgraph Collections["Tenant-Aware Collections"]
                COLL1["cortex-mem-tenant-a"]
                COLL2["cortex-mem-tenant-b"]
            end
        end
    end
    
    %% Relationships
    URI_SCHEME -- "Points to" --> Layout
    MD -- "Indexed by" --> AutoIndexer
    AutoIndexer -- "Generates" --> EMBEDDINGS
    EMBEDDINGS -- "Stored in" --> QDRANT
    METADATA -- "Filters" --> QDRANT
    
    style FS fill:#e1f5fe,stroke:#01579b
    style Vector fill:#fff3e0,stroke:#ef6c00
```

**Storage Strategy**:
- **Filesystem**: Provides durability, human-readable format (markdown), and easy backup/restore
- **Vector Store**: Enables semantic similarity search with metadata filtering and tenant isolation via collection suffixing (`cortex-mem-{tenant_id}`)
- **Synchronization**: Event-driven consistency through `AutoIndexer` and `SyncManager` components

### 3.4 Inter-Domain Communication

Domain modules communicate through **dependency injection** and **event-driven messaging**:

```mermaid
graph LR
    subgraph Communication["Inter-Domain Communication Patterns"]
        direction TB
        
        subgraph Sync["Synchronous Service Calls"]
            APP[Application Interface] -->|MemoryOperations| CORE[Core Infrastructure]
            AUTO[Automation] -->|Service Usage| CORE
            SEARCH[Search Engine] -->|Vector Queries| VECTOR[Vector Storage]
        end
        
        subgraph Async["Asynchronous Event Bus"]
            SESSION[Session Management] -->|SessionEvent::Closed| EVENTS[Event Bus]
            WATCHER[File Watcher] -->|FilesystemEvent| EVENTS
            EVENTS -->|Trigger| AUTO_MGR[Automation Manager]
        end
        
        subgraph Data["Data Flow"]
            EXTRACT[Extraction Engine] -->|ExtractedMemories| PROFILE[Profile Management]
            LAYER[Layer Management] -->|L0/L1 Summaries| SEARCH
        end
    end
    
    style EVENTS fill:#f9f,stroke:#333,stroke-width:2px
    style CORE fill:#f96,stroke:#333,stroke-width:2px
```

---

## 4. Component View

### 4.1 Core Infrastructure Components (`cortex-mem-core`)

The core crate implements a modular architecture with clear separation between automation, memory management, and infrastructure services:

```mermaid
graph TB
    subgraph CortexCore["cortex-mem-core Internal Architecture"]
        direction TB
        
        subgraph AutomationDomain["Automation Management Domain"]
            AUTO_MGR[Automation Manager<br/>Event Coordinator]
            WATCHER[File System Watcher<br/>Polling Monitor]
            INDEXER[Auto Indexer<br/>L0/L1/L2 Generator]
            EXTRACTOR[Auto Extractor<br/>Profile Updater]
            SYNC[Sync Manager<br/>Consistency Checker]
        end
        
        subgraph MemoryDomains["Memory Management Domains"]
            LAYER_MGR[Layer Manager<br/>Hierarchy Controller]
            SEARCH[Search Engine<br/>Vector Search + Ranking]
            SESSION[Session Manager<br/>Conversation State]
            PROFILE[Profile Manager<br/>User/Agent Profiles]
        end
        
        subgraph InfrastructureServices["Infrastructure Services"]
            FS[Filesystem Abstraction<br/>cortex:// URI]
            LLM[LLM Client<br/>OpenAI-compatible]
            EMBED[Embedding Client<br/>Vectorization]
            VECTOR[Vector Store<br/>Qdrant Client]
            EVENTS[Event Bus<br/>Async Channels]
        end
        
        subgraph DataLayer["Data & Types"]
            TYPES[Data Models<br/>Memory, Dimension, Filters]
            EXTRACTION_TYPES[Extraction Types<br/>Facts, Decisions, Entities]
            ERROR[Error Handling<br/>Domain Errors]
        end
    end
    
    %% Data Flow Relationships
    WATCHER -->|FilesystemEvent| AUTO_MGR
    SESSION -->|SessionEvent::Closed| AUTO_MGR
    AUTO_MGR -->|Trigger| INDEXER
    AUTO_MGR -->|Trigger| EXTRACTOR
    
    INDEXER -->|Generate| LAYER_MGR
    INDEXER -->|Store| VECTOR
    INDEXER -->|Read/Write| FS
    
    EXTRACTOR -->|Extract| LLM
    EXTRACTOR -->|Update| PROFILE
    EXTRACTOR -->|Write| FS
    
    SEARCH -->|Query| VECTOR
    SEARCH -->|Retrieve| LAYER_MGR
    SEARCH -->|Filter| FS
    
    SESSION -->|Persist| FS
    PROFILE -->|Persist| FS
    
    %% Infrastructure usage
    LAYER_MGR -->|Generate| LLM
    CORE_OPS[MemoryOperations Facade] -->|Coordinates| SEARCH
    CORE_OPS -->|Coordinates| SESSION
    CORE_OPS -->|Coordinates| PROFILE
    
    %% Event bus
    EVENTS -->|Broadcast| AUTO_MGR
    
    style AUTO_MGR fill:#f9f,stroke:#333,stroke-width:2px
    style SEARCH fill:#f9f,stroke:#333,stroke-width:2px
    style CORE_OPS fill:#ff9,stroke:#333,stroke-width:2px
```

### 4.2 Component Responsibility Division

#### 4.2.1 Automation Components

| Component | Responsibility | Key Algorithms |
|-----------|---------------|----------------|
| **AutomationManager** | Central event coordinator, manages task queue and throttling | Event filtering, batch scheduling |
| **FsWatcher** | Polls filesystem for changes, detects new/modified/deleted files | Polling with configurable intervals, checksum validation |
| **AutoIndexer** | Converts markdown content to vector embeddings, manages L0/L1 generation | Lazy summary generation, batch embedding |
| **AutoExtractor** | Post-session knowledge extraction, profile enrichment | LLM prompt engineering, deduplication (LCS similarity) |
| **SyncManager** | Full consistency checks between filesystem and vector store | Diff reconciliation, tenant-scoped sync |

#### 4.2.2 Memory Management Components

| Component | Responsibility | Key Algorithms |
|-----------|---------------|----------------|
| **LayerManager** | Three-tier hierarchy maintenance, cache management | LRU caching for L0/L1, on-demand generation |
| **VectorEngine** | Semantic search with adaptive thresholds | Weighted scoring (0.2×L0 + 0.3×L1 + 0.5×L2), intent detection |
| **SessionManager** | Conversation lifecycle, message aggregation | Timeline ordering, participant tracking |
| **ProfileManager** | Persistent user/agent knowledge bases | Category-based organization, importance scoring |

#### 4.2.3 Infrastructure Components

| Component | Responsibility | Key Features |
|-----------|---------------|--------------|
| **CortexFilesystem** | URI-based resource access, tenant isolation | `cortex://` scheme parsing, async I/O |
| **LLMClient** | OpenAI-compatible API wrapper, structured extraction | Prompt templating, JSON schema validation, fallback parsing |
| **EmbeddingClient** | Text vectorization | Batch processing, dimensionality management |
| **QdrantVectorStore** | Vector CRUD, similarity search | Tenant-aware collection naming, metadata filtering |
| **EventBus** | Async inter-component communication | Tokio mpsc channels, broadcast capabilities |

### 4.3 Interface Components

```mermaid
graph LR
    subgraph Interfaces["Application Interface Components"]
        direction TB
        
        subgraph CLI["cortex-mem-cli"]
            CMD[Command Parser<br/>clap]
            CMD_IMPL[Command Implementations<br/>search, session, add]
        end
        
        subgraph HTTP["cortex-mem-service"]
            AXUM[Axum Router]
            STATE[AppState<br/>MemoryOperations]
            HANDLERS[API Handlers<br/>/api/v2/*]
            MIDDLE[Middleware<br/>Tenant Extraction]
        end
        
        subgraph MCP["cortex-mem-mcp"]
            PROTO[MCP Protocol Handler]
            TOOLS[MCP Tools<br/>store_memory, query_memory]
            STDIO[Stdio Transport]
        end
        
        subgraph UI["cortex-mem-insights"]
            SVELTE[SvelteKit Router]
            STORES[Reactive Stores<br/>tenant.ts]
            API_CLIENT[API Client<br/>lib/api.ts]
            PAGES[Page Components<br/>Dashboard, Search, Memories]
        end
    end
    
    %% Dependencies
    CMD_IMPL -->|Uses| CORE[Core Library]
    HANDLERS -->|Uses| STATE
    STATE -->|Wraps| CORE
    TOOLS -->|Uses| CORE
    API_CLIENT -->|Calls| HTTP
    
    style CORE fill:#f96,stroke:#333
```

---

## 5. Key Processes

### 5.1 Semantic Memory Search Process (Primary Workflow)

The system's core value proposition—intelligent retrieval using the three-tier hierarchy:

```mermaid
sequenceDiagram
    participant Client as API/CLI Client
    participant Search as Search Engine
    participant Embed as Embedding Client
    participant Vector as Vector Store
    participant Layer as Layer Manager
    participant FS as Filesystem
    
    Client->>Search: Query (scope, limit, threshold)
    
    Search->>Search: Detect intent (Factual/Search/Relational/Temporal)
    Search->>Embed: Generate query embedding
    Search->>Layer: Load L0 summaries (if cached)
    
    Search->>Vector: Search L0 (coarse filter, 0.2 weight)
    Vector-->>Search: L0 candidates
    
    Search->>Vector: Search L1 (context refinement, 0.3 weight)
    Vector-->>Search: L1 candidates
    
    Search->>Vector: Search L2 (precise match, 0.5 weight)
    Vector-->>Search: L2 candidates
    
    alt No results above threshold
        Search->>Search: Apply degradation (0.5→0.4→0.3)
        Search->>Vector: Re-query with lower threshold
    end
    
    Search->>Search: Weighted scoring & ranking
    Search->>FS: Retrieve full content for top results
    Search->>Search: Generate snippets (100-char context)
    Search-->>Client: Ranked results with URIs & scores
```

**Process Characteristics**:
- **Progressive Precision**: L0 filters irrelevant documents quickly; L2 provides exact matches
- **Adaptive Thresholding**: Automatically degrades similarity thresholds if initial results are sparse
- **Weighted Aggregation**: L2 (detail) contributes most to final score (50%), followed by L1 (30%) and L0 (20%)

### 5.2 Memory Indexing and Synchronization Process

Background pipeline ensuring filesystem changes are searchable:

```mermaid
sequenceDiagram
    participant FS as OS Filesystem
    participant Watcher as File System Watcher
    participant AutoMgr as Automation Manager
    participant LayerMgr as Layer Manager
    participant Indexer as Auto Indexer
    participant LLM as LLM Client
    participant Vector as Qdrant Vector Store
    
    Note over FS,Vector: Automatic Indexing Pipeline
    
    FS->>Watcher: File modified/created (.md)
    Watcher->>AutoMgr: Emit FilesystemEvent
    AutoMgr->>AutoMgr: Queue for batch processing
    AutoMgr->>Indexer: Trigger indexing (throttled)
    
    Indexer->>FS: Read raw content (L2)
    Indexer->>LayerMgr: Request L0/L1 generation
    
    alt L0/L1 not cached
        LayerMgr->>LLM: Generate abstract (L0)
        LayerMgr->>LLM: Generate overview (L1)
        LayerMgr->>FS: Cache summaries
    end
    
    LayerMgr-->>Indexer: Return L0/L1/L2 content
    Indexer->>Indexer: Generate deterministic vector ID (URI + layer)
    Indexer->>Vector: Upsert vectors with metadata
    Note right of Vector: Tenant-aware<br/>collection naming<br/>cortex-mem-{tenant_id}
    
    Indexer->>FS: Update index state (checkpoint)
```

**Optimization Strategies**:
- **Batch Processing**: Accumulates changes before indexing to reduce LLM API costs
- **Lazy Generation**: Only generates L0/L1 when requested and not cached
- **Deterministic IDs**: Vector IDs derived from content URI + layer to prevent duplicates

### 5.3 Memory Extraction and Profiling Process

Post-session knowledge mining for personalization:

```mermaid
sequenceDiagram
    participant Session as Session Manager
    participant Events as Event Bus
    participant AutoMgr as Automation Manager
    participant Extractor as Auto Extractor
    participant LLM as LLM Client
    participant Profile as Profile Manager
    participant FS as Filesystem
    
    Session->>Events: Emit SessionEvent::Closed
    Events->>AutoMgr: Listen for closure events
    
    AutoMgr->>Extractor: Trigger extraction (if enabled)
    
    Extractor->>FS: Read conversation thread (recursive)
    Extractor->>FS: Load existing user/agent profiles
    Extractor->>LLM: Extract facts, decisions, entities<br/>with confidence scores
    
    LLM-->>Extractor: Structured memories (JSON)
    
    Extractor->>Extractor: Deduplication (LCS similarity >0.8)
    Extractor->>Extractor: Confidence filtering (>0.7)
    
    Extractor->>Profile: Merge into profiles (category-based)
    Profile->>Profile: Enforce limits (max 100 facts/category)
    Profile->>FS: Persist to cortex://user or cortex://agent
```

**Extraction Quality Assurance**:
- **Confidence Scoring**: LLM-assigned confidence scores filter low-quality extractions
- **Deduplication**: Longest Common Subsequence (LCS) algorithm prevents redundant facts
- **Category Limits**: Prevents profile bloat by limiting facts per category (e.g., personal_info, work_history)

### 5.4 Multi-Tenant Isolation Process

Ensures data segregation across tenants in SaaS deployments:

```mermaid
sequenceDiagram
    participant Request as Incoming Request
    participant Config as Configuration
    participant FS as Filesystem Layer
    participant Vector as Vector Store
    participant Profile as Profile Manager
    
    Request->>Request: Extract tenant_id (CLI arg, HTTP header, MCP context)
    Request->>Config: Propagate to all services
    
    par Filesystem Isolation
        Config->>FS: Map paths to /data/tenants/{tenant_id}/
        FS->>FS: URI resolution: cortex://session/{id} →<br/>/data/tenants/{tenant_id}/session/{id}
    and Vector Isolation
        Config->>Vector: Suffix collection: cortex-mem-{tenant_id}
        Vector->>Vector: Query only within tenant collection
    and Profile Isolation
        Config->>Profile: Scope lookups to tenant subdirectories
        Profile->>Profile: Filter: cortex://user/{id}?tenant={tenant_id}
    end
    
    Request->>Request: All operations scoped to tenant boundary
```

---

## 6. Technical Implementation

### 6.1 Core Module Implementation

#### 6.1.1 URI Scheme and Filesystem Abstraction

The `cortex://` URI scheme provides **location transparency**:

```rust
// URI Structure: cortex://{dimension}/{id}[/{subresource}]
// Examples:
// - cortex://session/uuid123/timeline/001.md
// - cortex://user/john_doe/profile.json
// - cortex://agent/tars/skills.json

pub struct CortexUri {
    pub dimension: Dimension,  // user, agent, session, resources
    pub id: String,
    pub path: Option<String>,
    pub tenant_id: Option<String>,
}

impl CortexFilesystem {
    pub async fn read(&self, uri: &CortexUri) -> Result<Bytes>;
    pub async fn write(&self, uri: &CortexUri, content: Bytes) -> Result<()>;
    pub async fn list(&self, uri: &CortexUri) -> Result<Vec<MemoryEntry>>;
}
```

**Tenant Isolation Implementation**:
- Physical path: `{data_dir}/tenants/{tenant_id}/{dimension}/{id}/`
- URI parsing extracts tenant from context or defaults to "default"
- All filesystem operations validate tenant scope before execution

#### 6.1.2 Vector Search Implementation

The search engine implements **layered retrieval with weighted scoring**:

```rust
pub struct VectorEngine {
    vector_store: Arc<dyn VectorStore>,
    layer_manager: Arc<LayerManager>,
    config: SearchConfig,
}

impl VectorEngine {
    pub async fn search(&self, query: SearchQuery) -> Result<SearchResults> {
        // 1. Generate embedding
        let embedding = self.embed(&query.text).await?;
        
        // 2. Intent detection for query optimization
        let intent = self.detect_intent(&query.text);
        
        // 3. Layered search
        let l0_results = self.search_layer(&embedding, Layer::L0, 0.2).await?;
        let l1_results = self.search_layer(&embedding, Layer::L1, 0.3).await?;
        let l2_results = self.search_layer(&embedding, Layer::L2, 0.5).await?;
        
        // 4. Weighted aggregation
        let mut combined = self.merge_results(l0_results, l1_results, l2_results);
        
        // 5. Adaptive thresholding if insufficient results
        if combined.len() < query.min_results {
            combined = self.degrade_thresholds(&embedding, intent).await?;
        }
        
        Ok(combined)
    }
}
```

**Scoring Algorithm**:
```
FinalScore = (0.2 × L0_Score) + (0.3 × L1_Score) + (0.5 × L2_Score)
```

#### 6.1.3 Event-Driven Architecture

The automation system uses **Tokio channels** for decoupled communication:

```rust
pub enum CortexEvent {
    FilesystemEvent(FsChange),
    SessionEvent(SessionChange),
    SystemEvent(SystemState),
}

pub struct AutomationManager {
    event_rx: mpsc::Receiver<CortexEvent>,
    task_queue: VecDeque<AutomationTask>,
}

impl AutomationManager {
    pub async fn run(&mut self) {
        while let Some(event) = self.event_rx.recv().await {
            match event {
                CortexEvent::FilesystemEvent(change) => {
                    self.queue_indexing(change);
                }
                CortexEvent::SessionEvent(SessionChange::Closed(id)) => {
                    self.trigger_extraction(id);
                }
                // ...
            }
        }
    }
}
```

### 6.2 Key Algorithm Design

#### 6.2.1 L0/L1 Summary Generation

**L0 (Abstract) Generation**:
- **Input**: Raw conversation markdown (L2)
- **Process**: LLM prompt with summarization instructions
- **Output**: Single paragraph (≤200 tokens) capturing essence
- **Caching**: Stored as `{uri}.abstract.md`

**L1 (Overview) Generation**:
- **Input**: Raw conversation or L0 summary
- **Process**: Structured extraction (key points, decisions, entities)
- **Output**: Markdown with YAML frontmatter
- **Caching**: Stored as `{uri}.overview.md`

#### 6.2.2 Deduplication Algorithm

For profile extraction deduplication:

```rust
fn is_duplicate(new_fact: &str, existing: &[Fact]) -> bool {
    for fact in existing {
        let similarity = lcs_similarity(new_fact, &fact.content);
        if similarity > 0.8 {
            return true;
        }
    }
    false
}

fn lcs_similarity(a: &str, b: &str) -> f64 {
    let lcs_len = longest_common_subsequence(a, b);
    2.0 * lcs_len as f64 / (a.len() + b.len()) as f64
}
```

### 6.3 Data Structure Design

#### 6.3.1 Core Domain Types

```rust
pub struct Memory {
    pub uri: CortexUri,
    pub layer: Layer,  // L0, L1, L2
    pub content: String,
    pub embedding: Option<Vec<f32>>,
    pub metadata: MemoryMetadata,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

pub struct MemoryMetadata {
    pub tenant_id: String,
    pub dimension: Dimension,
    pub entities: Vec<String>,
    pub importance: f32,  // 0.0 - 1.0
    pub source_hash: String,  // Content checksum
}

pub enum Dimension {
    User,
    Agent,
    Session,
    Resources,
}
```

#### 6.3.2 Extraction Types

```rust
pub struct ExtractedMemories {
    pub facts: Vec<ExtractedFact>,
    pub decisions: Vec<ExtractedDecision>,
    pub entities: Vec<ExtractedEntity>,
}

pub struct ExtractedFact {
    pub content: String,
    pub confidence: f32,
    pub category: FactCategory,  // personal, work, preference, etc.
    pub source_uri: CortexUri,
    pub timestamp: DateTime<Utc>,
}
```

### 6.4 Performance Optimization Strategies

| Strategy | Implementation | Impact |
|----------|---------------|---------|
| **Lazy Loading** | L0/L1 generated on-demand with filesystem caching | Reduces LLM API costs by 60-80% |
| **Batch Processing** | Accumulates file changes for 30s before indexing | Reduces vector store write operations |
| **Connection Pooling** | HTTP client reuse for LLM and Qdrant connections | Improves throughput under load |
| **Caching Layer** | In-memory LRU for L0/L1 summaries | Reduces filesystem I/O |
| **Deterministic Vector IDs** | Hash-based ID generation prevents duplicate storage | Eliminates vector collisions |
| **Adaptive Thresholds** | Degrades similarity requirements for ambiguous queries | Improves recall without sacrificing precision |

---

## 7. Deployment Architecture

### 7.1 Runtime Environment Requirements

**System Requirements**:
- **OS**: Linux (Ubuntu 22.04+), macOS (13+), Windows (WSL2 recommended)
- **Rust**: 1.75+ (async/await optimizations)
- **Qdrant**: 1.7+ (vector database) - Local or remote
- **LLM Provider**: OpenAI-compatible API endpoint
- **Resources**: 
  - Minimum: 4GB RAM, 2 CPU cores
  - Recommended: 8GB RAM, 4 CPU cores, SSD storage

**Dependencies**:
```toml
[workspace.dependencies]
tokio = { version = "1.35", features = ["full"] }
axum = "0.7"
qdrant-client = "1.8"
serde = { version = "1.0", features = ["derive"] }
```

### 7.2 Deployment Topology

#### 7.2.1 Single Node Deployment (Development/Small Scale)

```mermaid
graph LR
    subgraph SingleNode["Single Node Deployment"]
        direction TB
        
        subgraph Binaries["Executable Binaries"]
            CLI["cortex-mem-cli"]
            SERVICE["cortex-mem-service"]
            MCP["cortex-mem-mcp"]
        end
        
        subgraph Shared["Shared Resources"]
            CORE_LIB["Core Library"]
            DATA_DIR["/data"]
        end
        
        FS[("Local Filesystem")]
        QDRANT[("Qdrant Local\n:6334")]
    end
    
    LLM[("LLM API")]
    CLIENTS["HTTP/MCP Clients"]
    
    CLI --> CORE_LIB
    SERVICE --> CORE_LIB
    MCP --> CORE_LIB
    CORE_LIB --> FS
    CORE_LIB --> QDRANT
    CORE_LIB --> LLM
    CLIENTS --> SERVICE
    CLIENTS --> MCP
    
    style SingleNode fill:#e8f5e9,stroke:#2e7d32
```

**Configuration**:
- Qdrant runs as separate process or Docker container
- All binaries share the same data directory
- Tenant isolation via subdirectory scoping

#### 7.2.2 Multi-Tenant Production Deployment

```mermaid
graph TB
    subgraph LoadBalancer["Load Balancer / Reverse Proxy"]
        NGINX[NGINX / Traefik]
    end
    
    subgraph AppCluster["Application Cluster"]
        INST1[cortex-mem-service<br/>Instance 1]
        INST2[cortex-mem-service<br/>Instance 2]
        INST3[cortex-mem-service<br/>Instance 3]
    end
    
    subgraph Storage["Storage Layer"]
        QDRANT_CLUSTER[(Qdrant Cluster<br/>Distributed)]
        SHARED_FS[(Shared Volume<br/>NFS/GlusterFS)]
    end
    
    subgraph External["External Services"]
        LLM_API[LLM API Provider]
        MONITORING[Prometheus/Grafana]
    end
    
    NGINX -->|Route by tenant| INST1
    NGINX --> INST2
    NGINX --> INST3
    
    INST1 --> QDRANT_CLUSTER
    INST1 --> SHARED_FS
    INST2 --> QDRANT_CLUSTER
    INST2 --> SHARED_FS
    INST3 --> QDRANT_CLUSTER
    INST3 --> SHARED_FS
    
    INST1 --> LLM_API
    INST2 --> LLM_API
    INST3 --> LLM_API
    
    style AppCluster fill:#e3f2fd,stroke:#1565c0
```

**Scaling Considerations**:
- **Horizontal**: Stateless application servers behind load balancer
- **Vertical**: Qdrant cluster supports sharding for large vector collections
- **Storage**: Shared filesystem required for multi-node deployments (NFS, S3-compatible storage)
- **Tenant Routing**: Reverse proxy routes requests to appropriate instances based on tenant ID header

### 7.3 Monitoring and Observability

**Health Check Endpoints**:
- `GET /health` - Service liveness
- `GET /health/ready` - Dependency readiness (Qdrant, LLM connectivity)
- `GET /api/v2/tenants/{id}/stats` - Per-tenant storage metrics

**Key Metrics**:
| Metric | Type | Description |
|--------|------|-------------|
| `cortex_search_latency` | Histogram | Query response time by layer |
| `cortex_indexing_queue` | Gauge | Pending files for indexing |
| `cortex_llm_requests` | Counter | LLM API calls (token usage) |
| `cortex_tenant_memory_bytes` | Gauge | Storage usage per tenant |
| `cortex_event_bus_lag` | Gauge | Event processing delay |

**Logging Strategy**:
- **Structured Logging**: JSON format with tenant_id correlation
- **Log Levels**: 
  - INFO: Business operations (search, store, extract)
  - DEBUG: Automation triggers, cache hits/misses
  - ERROR: LLM failures, storage errors

### 7.4 Security Considerations

**Tenant Isolation**:
- **Validation**: All requests validate tenant ID against authorized list
- **Collection Namespacing**: Strict suffix pattern prevents cross-tenant queries
- **Filesystem Sandboxing**: Path resolution rejects paths outside tenant directory

**API Security**:
- **Authentication**: Bearer token validation at reverse proxy level
- **Authorization**: Tenant-scoped access tokens
- **Rate Limiting**: Per-tenant limits on LLM calls (cost control)

**Data Protection**:
- **Encryption at Rest**: Filesystem encryption (LUKS) or managed disk encryption
- **Encryption in Transit**: TLS 1.3 for all HTTP APIs and Qdrant connections
- **PII Handling**: Automatic PII detection in extraction engine with redaction options

---

## 8. Architectural Recommendations

### 8.1 Scalability Roadmap

1. **Short Term** (Current):
   - Optimize batch sizes for indexing (tune 30s window based on throughput)
   - Implement connection pooling for Qdrant (currently per-request)

2. **Medium Term**:
   - Implement read replicas for Qdrant search operations
   - Add Redis caching layer for L0 summaries (reduce filesystem I/O)
   - sharding strategy for high-volume tenants

3. **Long Term**:
   - Separate automation workers (indexing/extraction) into standalone services
   - Implement event streaming (Kafka/Redis Streams) for cross-service communication
   - Tiered storage: Hot (SSD) → Warm (HDD) → Cold (S3) for old sessions

### 8.2 Technical Debt and Gaps

| Gap | Risk | Mitigation |
|-----|------|------------|
| **Example App Duplication** (`cortex-mem-tars`) | DRY violation, maintenance overhead | Refactor to use published crates; reduce to example code only |
| **Frontend Type Safety** | API drift between Rust and TypeScript | Implement OpenAPI generation (utoipa) → TypeScript client generation |
| **Filesystem Polling** | CPU overhead on large directories | Migrate to platform-specific notify mechanisms (inotify/fsevents) |
| **No Rate Limiting** | LLM cost exposure, abuse | Add Tower middleware for per-tenant rate limiting |

### 8.3 Development Guidelines

**For Core Library Development**:
- Maintain strict API backward compatibility in `cortex-mem-core`
- All storage operations must be async and cancelable (Tokio cancellation tokens)
- Tenant ID must propagate through entire call chain (use structured logging context)

**For Interface Development**:
- CLI commands should follow POSIX conventions with JSON output options (`--output json`)
- HTTP handlers must validate tenant extraction before calling core operations
- MCP tools require strict JSON schema validation and descriptive error messages

**Testing Strategy**:
- Unit tests: Mock filesystem and vector store implementations
- Integration tests: Docker Compose stack with Qdrant and mock LLM server
- Load tests: Simulate 100 concurrent tenants with 10K sessions each

---

**Document Status**: Draft  
**Review Cycle**: Quarterly or upon major version release  
**Stakeholders**: Architecture Board, Engineering Teams, DevOps

---

*This architecture documentation follows the C4 Model (Context, Container, Component, Code) visualization approach developed by Simon Brown.*
