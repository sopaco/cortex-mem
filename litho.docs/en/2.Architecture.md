# Cortex-Mem System Architecture Documentation

**Document Version**: 1.0  
**Generation Date**: 2026-02-17 16:32:40 (UTC)  
**Classification**: Architecture Overview (C4 Model - Container Level)  
**Confidence Level**: 95%

---

## 1. Architecture Overview

### 1.1 Design Philosophy

Cortex-Mem implements a **layered, domain-driven architecture** designed to solve the fundamental context window limitation problem in Large Language Model (LLM) applications. The system adopts **Hexagonal Architecture** (Ports and Adapters) principles to ensure business logic remains isolated from infrastructure concerns, enabling portability across different deployment environments and AI frameworks.

The architecture centers on four core design principles:

1. **Semantic-First Storage**: All memory content is vectorized using embeddings to enable semantic similarity search rather than lexical matching, allowing AI agents to retrieve context based on conceptual relevance.

2. **Progressive Disclosure**: The three-tier abstraction layer system (L0 Abstract → L1 Overview → L2 Detail) optimizes token usage by providing progressively detailed context only when necessary, respecting LLM context window constraints.

3. **Multi-Dimensional Organization**: Memory is organized across three orthogonal dimensions—User, Agent, and Session—enabling precise scoping and multi-tenancy isolation while maintaining cross-dimensional search capabilities.

4. **Polyglot Interoperability**: The computational core is implemented in Rust for performance and safety, while the presentation layer uses TypeScript/SvelteKit for rapid UI development, connected via well-defined REST and MCP protocols.

### 1.2 Core Architecture Patterns

| Pattern | Implementation | Business Value |
|---------|---------------|----------------|
| **CQRS** | Separate command (write) and query (read) paths for memory operations; vector indexing is asynchronous from filesystem persistence | High write throughput with consistent search availability |
| **Event-Driven Automation** | File system watchers trigger automatic indexing and memory extraction workflows | Zero-latency background processing without blocking user operations |
| **Virtual Filesystem Gateway** | `cortex://` URI scheme abstracts physical storage locations | Portability, version control compatibility, and multi-tenancy isolation |
| **Tiered Caching** | L0/L1 layer generation is cached to filesystem to avoid redundant LLM calls | Cost optimization and latency reduction for repeated context requests |
| **Dependency Inversion** | Core domain has no dependencies on interface layers; all dependencies point inward | Testability, framework independence, and long-term maintainability |

### 1.3 Technology Stack Overview

```mermaid
flowchart TB
    subgraph CoreRuntime["Core Runtime (Rust)"]
        Tokio[Tokio Async Runtime]
        Axum[Axum Web Framework]
        Serde[Serde Serialization]
        Rig[Rig LLM Framework]
        QdrantClient[Qdrant Vector Client]
    end
    
    subgraph Presentation["Presentation Layer (TypeScript)"]
        Svelte[Svelte 5 SPA<br/>Pure Frontend]
    end
    
    subgraph External["External Infrastructure"]
        Qdrant[(Qdrant Vector DB)]
        LLM[OpenAI-Compatible LLM API]
        Embedding[Embedding Service API]
        FS[(Local Filesystem)]
    end
    
    cortex-mem-core --> Tokio
    cortex-mem-service --> Axum
    cortex-mem-core --> Serde
    cortex-mem-rig --> Rig
    cortex-mem-core --> QdrantClient
    
    cortex-mem-insights -->|REST API| Svelte
    
    cortex-mem-core --> Qdrant
    cortex-mem-core --> LLM
    cortex-mem-core --> Embedding
    cortex-mem-core --> FS
    
    style CoreRuntime fill:#e3f2fd,stroke:#1565c0
    style Presentation fill:#e8f5e9,stroke:#2e7d32
    style External fill:#fff3e0,stroke:#ef6c00
```

---

## 2. System Context (C4 Level 1)

### 2.1 System Positioning and Value Proposition

Cortex-Mem operates as **persistent memory infrastructure** for AI agents and LLM applications, positioned between raw LLM APIs and application-specific agent logic. The system transforms ephemeral conversation state into structured, searchable, and reusable long-term memory.

**Core Value Propositions**:
- **Context Window Extension**: Overcomes token limitations through semantic retrieval of relevant historical context
- **Multi-Tenancy Isolation**: Supports isolated memory spaces for different users and agents within a single deployment
- **Automated Knowledge Extraction**: LLM-powered analysis automatically distills conversation facts, preferences, and decisions into structured memories
- **Framework Agnostic**: Supports integration via CLI, HTTP REST, Model Context Protocol (MCP), and direct Rust library embedding

### 2.2 User Roles and Interaction Models

```mermaid
flowchart TB
    subgraph Users["User Personas"]
        Dev[(AI Agent Developers<br/><i>Primary Users</i>)]
        EndUser[(End Users<br/><i>Beneficiaries</i>)]
        Admin[(System Administrators<br/><i>Operators</i>)]
    end
    
    subgraph CortexMem["Cortex-Mem System"]
        direction TB
        Core[Core Memory Engine]
        Interfaces[Interface Adapters<br/>CLI / REST / MCP]
        Insights[Monitoring & Insights<br/>Web Dashboard]
    end
    
    subgraph External["External Systems"]
        Qdrant[(Qdrant Vector DB)]
        LLM[LLM Provider]
        Embed[Embedding Service]
    end
    
    Dev -->|Rust Library / CLI / API| Interfaces
    EndUser -->|Web UI| Insights
    Admin -->|Dashboard / Health APIs| Insights
    
    Interfaces -->|Vector Operations| Qdrant
    Core -->|Embeddings| Embed
    Core -->|Completion/Extraction| LLM
    Core -->|cortex:// Storage| FS[(Filesystem)]
    
    Insights -->|Metrics| Interfaces
    
    style CortexMem fill:#e1f5ff,stroke:#01579b,stroke-width:3px
    style Users fill:#e8f5e9,stroke:#2e7d32
    style External fill:#fff3e0,stroke:#ef6c00
```

**User Role Details**:

| Role | Interaction Mode | Primary Needs | Technical Interface |
|------|-----------------|---------------|---------------------|
| **AI Agent Developers** | Programmatic integration | Vector search, multi-tenancy, tiered layers | Rust crates (`cortex-mem-core`), HTTP REST API, MCP tools |
| **End Users** | Conversational AI with memory | Persistent history, preference recall, session continuity | Indirect via AI agents; Direct via Web Insights dashboard |
| **System Administrators** | Operations and monitoring | Health metrics, configuration management, optimization | Web Insights dashboard, CLI admin commands, log aggregation |

### 2.3 External System Dependencies

The system maintains strict boundaries with external infrastructure, treating all LLM and vector storage services as replaceable adapters:

1. **Qdrant Vector Database**: Stores high-dimensional embeddings (default 1536-dim) with HNSW indexing for approximate nearest neighbor search. Connection via gRPC/HTTP with configurable URL and API key authentication.

2. **LLM Provider (OpenAI-compatible)**: Powers memory extraction, content summarization, and tiered layer generation. Supports OpenAI, Azure OpenAI, or local inference servers (Ollama, LM Studio) via configurable base URL and model parameters.

3. **Embedding Service API**: Converts text to vector representations for semantic indexing. Typically OpenAI's `text-embedding-3-small` or compatible alternatives.

4. **Filesystem Storage**: Local or network-attached storage for memory content, session timelines, and generated abstraction layers. Uses virtual URI scheme (`cortex://`) mapped to directory structure for portability.

### 2.4 System Boundary Definition

**Included in System Boundary**:
- `cortex-mem-core`: Rust library containing domain logic, memory operations, vector search, LLM clients
- `cortex-mem-cli`: Command-line interface application
- `cortex-mem-service`: HTTP REST API server (Axum-based)
- `cortex-mem-mcp`: Model Context Protocol server for AI assistant integration
- `cortex-mem-insights`: Pure frontend Svelte 5 SPA web application for monitoring and analytics
- `cortex-mem-tools`: Agent tool definitions and high-level operations facade
- `cortex-mem-config`: Cross-cutting configuration management
- `cortex-mem-rig`: Rig framework integration crate (placeholder for future expansion)

**Excluded from System Boundary**:
- External LLM service providers (OpenAI, Azure, local inference servers)
- Qdrant vector database instances (external service dependency)
- Third-party embedding API services
- Operating system filesystem drivers and network infrastructure

---

## 3. Container View (C4 Level 2)

### 3.1 Domain Module Architecture

Cortex-Mem implements **Clean Architecture** with four concentric layers, enforcing the Dependency Rule (dependencies point inward):

```mermaid
flowchart TB
    subgraph Infrastructure["Infrastructure Layer"]
        Config[cortex-mem-config<br/>Configuration Management]
        Init[(System Initialization<br/>Logging, Data Dirs)]
    end
    
    subgraph CoreDomain["Core Domain Layer"]
        CoreLib[cortex-mem-core<br/>Memory, Search, LLM, Sessions]
        ToolsLib[cortex-mem-tools<br/>Agent Tool Definitions]
        RigInt[cortex-mem-rig<br/>Rig Framework Integration]
    end
    
    subgraph InterfaceAdapters["Interface Adapters Layer"]
        CLI[cortex-mem-cli<br/>Command Line Interface]
        Service[cortex-mem-service<br/>HTTP REST API]
        MCP[cortex-mem-mcp<br/>Model Context Protocol]
    end
    
    subgraph Presentation["Presentation Layer"]
        Insights[cortex-mem-insights<br/>Svelte 5 SPA<br/>Pure Frontend]
    end
    
    subgraph External["External Services"]
        Qdrant[(Qdrant Vector DB)]
        LLM[LLM/Embedding APIs]
        FS[(Filesystem)]
    end
    
    %% Dependencies - Flow Inward
    CLI --> CoreLib
    CLI --> Config
    Service --> CoreLib
    Service --> Config
    MCP --> CoreLib
    MCP --> ToolsLib
    
    ToolsLib --> CoreLib
    RigInt --> ToolsLib
    RigInt --> CoreLib
    
    Insights -->|HTTP REST| Service
    Insights -->|Direct Integration| CoreLib
    
    CoreLib --> Qdrant
    CoreLib --> LLM
    CoreLib --> FS
    CoreLib --> Init
    
    style CoreDomain fill:#e3f2fd,stroke:#1565c0,stroke-width:3px
    style InterfaceAdapters fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style Presentation fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style Infrastructure fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
```

### 3.2 Domain Module Division

The system is organized into **nine bounded domains** following Domain-Driven Design principles:

#### Core Business Domains (High Business Value)

**1. Memory Management Domain** (`cortex-mem-core`, `cortex-mem-tools`)
- **Responsibility**: Multi-dimensional memory storage, retrieval, and lifecycle management
- **Key Capabilities**: URI-based addressing (`cortex://`), tiered layer management (L0/L1/L2), automated extraction orchestration
- **Complexity**: High (9/10) - Central business logic hub
- **Sub-modules**: Core API Surface, Type Definitions, Memory Operations, Extraction Engine, Agent Tool Integration

**2. Vector Search Domain** (`cortex-mem-core/src/search`, `/vector_store`, `/embedding`)
- **Responsibility**: Semantic similarity search via vector embeddings
- **Key Capabilities**: Embedding generation, Qdrant integration, similarity scoring, metadata filtering
- **Complexity**: High (8/10) - Mathematical and infrastructure complexity
- **Sub-modules**: Vector Store (Qdrant client), Embedding Client, Vector Search Engine

**3. LLM Integration Domain** (`cortex-mem-core/src/llm`, `/layers`, `cortex-mem-rig`)
- **Responsibility**: Abstraction over LLM providers for generation and analysis tasks
- **Key Capabilities**: Prompt engineering, structured extraction, tiered layer generation (abstracts/overviews), Rig framework integration
- **Complexity**: High (8/10) - Non-deterministic AI interactions and prompt management
- **Sub-modules**: LLM Client, Prompt Templates, Extraction Types, Layer Generation, Rig Integration

**4. Session Management Domain** (`cortex-mem-core/src/session`)
- **Responsibility**: Conversation lifecycle management and timeline tracking
- **Key Capabilities**: Session creation, message accumulation, timeline persistence, participant tracking
- **Complexity**: Medium (7/10) - State management and temporal organization
- **Sub-modules**: Session Manager, Timeline Management, Participant Management, Auto Indexer

#### Infrastructure Domains (Technical Capabilities)

**5. Storage Infrastructure Domain** (`cortex-mem-core/src/filesystem`, `/init`, `/logging`)
- **Responsibility**: Physical persistence through virtual filesystem abstraction
- **Key Capabilities**: `cortex://` URI scheme implementation, cross-platform file I/O, system initialization, logging infrastructure
- **Complexity**: Medium (6/10) - OS-level operations and path management
- **Sub-modules**: Filesystem Operations, URI Parser, System Initialization, Logging System

**6. Automation Domain** (`cortex-mem-core/src/automation`)
- **Responsibility**: Background processing and event-driven workflows
- **Key Capabilities**: File system watching, automatic indexing, memory extraction triggers, synchronization between filesystem and vector store
- **Complexity**: Medium (6/10) - Concurrency and event handling
- **Sub-modules**: File Watcher and Sync, Auto Extractor, Automation Configuration

**7. Configuration Management Domain** (`cortex-mem-config`)
- **Responsibility**: Centralized, type-safe configuration across all components
- **Key Capabilities**: TOML parsing, environment variable integration, data directory resolution, cross-platform path handling
- **Complexity**: Low (5/10) - Data structures and file parsing
- **Sub-modules**: Configuration Library

#### Application Domains (User Interfaces)

**8. Interface Layer Domain** (`cortex-mem-cli`, `cortex-mem-service`, `cortex-mem-mcp`)
- **Responsibility**: Multiple transport protocols for system access
- **Key Capabilities**: CLI argument parsing, HTTP REST routing, MCP protocol implementation, request validation
- **Complexity**: Medium (7/10) - Protocol handling and adapter logic
- **Sub-modules**: CLI Application, REST API Service, MCP Server

**9. Monitoring and Insights Domain** (`cortex-mem-insights`)
- **Responsibility**: Observability, analytics, and management web interface
- **Key Capabilities**: System health monitoring, memory statistics visualization, tenant management, semantic search interface
- **Complexity**: Medium (6/10) - Pure frontend SPA complexity
- **Sub-modules**: Web Dashboard Frontend (Svelte 5), API Client, Frontend State Management

### 3.3 Storage Architecture Design

Cortex-Mem implements a **polyglot persistence** strategy optimized for different access patterns:

```mermaid
flowchart LR
    subgraph Storage["Storage Strategy"]
        subgraph Hot["Hot Path (Latency Sensitive)"]
            VS[(Qdrant Vector DB<br/>HNSW Index)]
            Cache[Layer Cache<br/>L0/L1 Generated Content]
        end
        
        subgraph Warm["Warm Path (Throughput)"]
            FS[(Filesystem<br/>Markdown/JSON)]
            Session[Session Timelines<br/>Conversation History]
        end
        
        subgraph Cold["Cold Path (Archive)"]
            Backup[(Backup/Export<br/>Portable Format)]
        end
    end
    
    subgraph Access["Access Patterns"]
        Vector[Semantic Search<br/>Vector Similarity]
        Metadata[Metadata Filtering<br/>Scope/Category/Date]
        Content[Full Content Retrieval<br/>URI Resolution]
        Tiered[Tiered Context Loading<br/>L0→L1→L2]
    end
    
    Vector --> VS
    Metadata --> VS
    Content --> FS
    Tiered --> Cache
    Tiered --> FS
    
    style Hot fill:#ffebee,stroke:#c62828
    style Warm fill:#fff3e0,stroke:#ef6c00
    style Cold fill:#e8f5e9,stroke:#2e7d32
```

**URI Scheme and Virtual Filesystem**:

The `cortex://` URI scheme provides location transparency:
```
cortex://{dimension}/{scope}/{category}/{id}
```

**Mapping Strategy**:
- **Dimension**: `user`, `agent`, `session` (enables multi-tenancy)
- **Scope**: Tenant identifier (UUID or semantic name)
- **Category**: `memories`, `profiles`, `entities`, `events`, `cases`
- **ID**: Unique memory identifier (UUID v4)

**Physical Layout**:
```yaml
data_dir/
├── user/
│   └── {user_id}/
│       ├── memories/
│       ├── profiles/
│       └── entities/
├── agent/
│   └── {agent_id}/
│       ├── memories/
│       └── profiles/
└── session/
    └── {session_id}/
        ├── timeline.md
        ├── memories/
        └── layers/
            ├── L0_abstract.json
            └── L1_overview.json
```

### 3.4 Inter-Domain Communication Patterns

**Synchronous Communication** (Request/Response):
- Interface Layer → Core Domain: Direct function calls via Rust crate dependencies
- Core Domain → External Services: Async HTTP/gRPC calls to LLM and Qdrant APIs
- Insights → REST API: HTTP client calls for data aggregation

**Asynchronous Communication** (Event-Driven):
- File System Watcher → Auto Extractor: File change events trigger background extraction
- Session Manager → Auto Indexer: New messages queued for vector indexing
- Memory Operations → Layer Generator: On-demand layer generation with filesystem caching

**Data Flow Integrity**:
The system maintains **eventual consistency** between filesystem (source of truth) and vector database (search index) through the automation domain's synchronization capabilities.

---

## 4. Component View (C4 Level 3)

### 4.1 Core Functional Components

#### 4.1.1 Memory Management Components

```mermaid
flowchart TB
    subgraph MemoryDomain["Memory Management Domain"]
        API[Core API Surface<br/>Trait Definitions]
        Types[Type System<br/>Memory/Dimension/Category]
        Ops[Memory Operations<br/>CRUD Facade]
        Extract[Extraction Engine<br/>LLM-Powered Analysis]
        Tools[Agent Tools<br/>Search/Store/Retrieve]
    end
    
    subgraph Dependencies["External Dependencies"]
        VS[Vector Search Domain]
        LLM[LLM Integration Domain]
        FS[Filesystem Operations]
    end
    
    API --> Types
    Ops --> API
    Ops --> VS
    Ops --> LLM
    Ops --> FS
    Extract --> LLM
    Extract --> Types
    Tools --> Ops
    
    style MemoryDomain fill:#e3f2fd,stroke:#1565c0
    style Dependencies fill:#fff3e0,stroke:#ef6c00
```

**Key Component Details**:

| Component | Responsibility | Key Interfaces |
|-----------|---------------|----------------|
| **Memory Operations** (`cortex-mem-tools/src/operations.rs`) | High-level facade coordinating storage, search, and LLM operations | `store_memory()`, `search_memories()`, `get_layer()` |
| **Type Definitions** (`cortex-mem-core/src/types.rs`) | Domain model: `Memory` struct, `Dimension` enum, `Category` classification, URI mapping | `Memory`, `Dimension`, `Filters`, `MemoryId` |
| **Extraction Engine** (`cortex-mem-core/src/extraction/`) | Automated content analysis using LLM prompts to extract facts, preferences, entities | `extract_conversation()`, `classify_memory()`, `generate_profile()` |
| **Agent Tools** (`cortex-mem-tools/src/tools/`) | Tool definitions for LLM agent integration (Rig/LangChain compatible) | `SearchTool`, `StoreTool`, `RetrieveTool` |

#### 4.1.2 Vector Search Components

```mermaid
flowchart LR
    subgraph VectorDomain["Vector Search Domain"]
        EC[Embedding Client<br/>Text → Vector]
        VS[Vector Store<br/>Qdrant Interface]
        SE[Search Engine<br/>Similarity + Filtering]
    end
    
    subgraph External["External Services"]
        Qdrant[(Qdrant DB)]
        EmbedAPI[Embedding API]
    end
    
    EC -->|HTTP/REST| EmbedAPI
    VS -->|gRPC/HTTP| Qdrant
    SE --> EC
    SE --> VS
    
    style VectorDomain fill:#e3f2fd,stroke:#1565c0
    style External fill:#fff3e0,stroke:#ef6c00
```

**Vector Search Pipeline**:
1. **Embedding Client**: Batches text content for efficient API usage; supports chunked processing for large documents
2. **Vector Store**: Manages Qdrant collections, point upserts, and payload indexing; handles connection pooling and retry logic
3. **Search Engine**: Orchestrates vector similarity search with metadata filters (scope, category, date ranges), calculates relevance scores, and ranks results

#### 4.1.3 LLM Integration Components

```mermaid
flowchart TB
    subgraph LLMDomain["LLM Integration Domain"]
        Client[LLM Client<br/>Completion & Extraction]
        Prompts[Prompt Templates<br/>System & User Prompts]
        Layers[Layer Generator<br/>L0/L1 Summarization]
        Types[Extraction Types<br/>Structured Output]
    end
    
    subgraph Provider["LLM Provider"]
        OpenAI[OpenAI/Azure API]
        Local[Local Inference]
    end
    
    Client -->|JSON Schema| Types
    Client -->|Template| Prompts
    Layers --> Client
    Client -->|HTTP/REST| OpenAI
    Client -->|HTTP/REST| Local
    
    style LLMDomain fill:#e3f2fd,stroke:#1565c0
    style Provider fill:#fff3e0,stroke:#ef6c00
```

**Tiered Layer Generation Logic**:
- **L0 Abstract Generator**: Condenses content to 1-2 sentences using extractive summarization prompts; stored as `abstract.json`
- **L1 Overview Generator**: Creates structured summaries (bullet points, key facts) using abstractive summarization; stored as `overview.json`
- **L2 Detail**: Raw content stored as Markdown or original format; not processed through LLM to preserve fidelity

### 4.2 Technical Support Components

#### 4.2.1 Session Management Components

- **Session Manager**: Handles session lifecycle (create, update, close), metadata tracking (participant counts, message statistics), and scope isolation
- **Timeline Manager**: Organizes messages chronologically with role tracking (user/assistant/system), supports branching conversations via parent references
- **Auto Indexer**: Background worker that converts session messages to vector embeddings; implements deduplication to prevent redundant indexing

#### 4.2.2 Automation Components

- **File Watcher**: Uses OS-specific filesystem notifications (inotify/kqueue/FSEvents) to detect new session files or memory updates
- **Auto Extractor**: Triggered by file watcher events; batches recent messages and invokes LLM extraction prompts
- **Sync Engine**: Reconciles filesystem state with vector database state; handles retries for failed vector operations

#### 4.2.3 Configuration Components

- **Config Loader**: TOML parsing with environment variable substitution; supports per-environment overrides (dev/staging/prod)
- **Path Resolver**: Cross-platform data directory resolution following XDG Base Directory Specification on Linux, standard locations on macOS/Windows
- **Validation**: Schema validation for LLM configurations (model names, temperature, max_tokens) and connection strings

### 4.3 Component Interaction Relationships

**Critical Dependency Paths** (Validated):

1. **CLI → Core → Vector Store**: `cortex-mem-cli` → `cortex-mem-core::vector_store` → Qdrant
2. **Search Pipeline**: `search::vector_engine` → `embedding::client` + `vector_store::qdrant`
3. **Layer Generation**: `layers::manager` → `llm::client` + `filesystem::operations`
4. **Session Indexing**: `automation::indexer` → `session::manager` + `vector_store`

**Dependency Inversion Compliance**:
- ✅ **Interface Layer** depends only on Core Domain and Config (inward dependency)
- ✅ **Core Domain** isolates business logic; external dependencies (Qdrant, LLM) are injected via ports
- ✅ **Infrastructure** (Automation, Storage) depends on Core types but implements interfaces defined by Core

**Identified Architectural Notes**:
1. **Insights Pure Frontend Architecture**: `cortex-mem-insights` is implemented as a pure frontend SPA that communicates exclusively with `cortex-mem-service` via REST API, maintaining clean architectural boundaries.
2. **Rig Framework Placeholder**: `cortex-mem-rig` exists as a separate crate but has minimal implementation surface. *Recommendation*: Complete Rig tool provider implementation or merge into `cortex-mem-tools`.

---

## 5. Key Processes

### 5.1 Core Functional Processes

#### 5.1.1 Memory Semantic Storage and Retrieval Flow (CQRS Pattern)

```mermaid
sequenceDiagram
    participant User
    participant CLI as CLI/API/MCP
    participant MM as Memory Mgmt Domain
    participant EC as Embedding Client
    participant VS as Vector Store
    participant FS as Filesystem Ops
    
    User->>CLI: Store Memory Request<br/>(content + metadata)
    CLI->>MM: create_memory(content, metadata)
    MM->>MM: Validate & Create Memory Object<br/>(Generate UUID, timestamp)
    
    par Parallel Processing Paths
        MM->>EC: generate_embedding(content)
        EC-->>MM: Vector (1536-dim)
        MM->>VS: store_vector(id, vector, metadata)
        VS->>VS: Persist to Qdrant<br/>(HNSW Index Update)
    and Filesystem Persistence
        MM->>FS: write_file(cortex://uri, content)
        FS->>FS: Persist to Local FS<br/>(Markdown/JSON)
    end
    
    MM-->>CLI: Confirm with URI & ID
    CLI-->>User: Success Response
    
    Note over User,FS: Query Flow (Separate Path)
    User->>CLI: Search Query (natural language)
    CLI->>MM: search(query, filters)
    MM->>EC: generate_embedding(query)
    EC-->>MM: Query Vector
    MM->>VS: similarity_search(vector, filters)
    VS-->>MM: Top-K IDs + Scores
    MM->>FS: retrieve_full_content(ids)
    FS-->>MM: Memory Content
    MM-->>CLI: Ranked Results
    CLI-->>User: Search Results
```

**Process Characteristics**:
- **Write Path**: Embeddings and filesystem writes happen in parallel for latency optimization
- **Read Path**: Vector search identifies candidates; filesystem retrieves full content (CQRS separation)
- **Consistency Model**: Eventual consistency between vector index and filesystem; automation domain handles reconciliation

#### 5.1.2 Automated Memory Extraction Flow (Event-Driven)

```mermaid
flowchart TD
    A[Session Timeline<br/>New Messages] -->|File Watcher Event| B[Auto Extractor]
    B --> C[Batch Messages<br/>Sliding Window]
    C --> D[LLM Analysis<br/>Extraction Prompt]
    D --> E{Extraction Types}
    E -->|Facts| F[Memory Mgmt<br/>Store as Memory]
    E -->|Preferences| F
    E -->|Entities| F
    E -->|Decisions| F
    F --> G[Generate Embeddings]
    G --> H[Vector Store<br/>Index for Search]
    F --> I[Filesystem<br/>Persist Content]
    H --> J[Update Index Status]
    I --> J
    J --> K[Notify Completion<br/>Stats/Metrics]
    
    style A fill:#e8f5e9,stroke:#2e7d32
    style B fill:#fff3e0,stroke:#ef6c00
    style D fill:#e3f2fd,stroke:#1565c0
    style F fill:#e3f2fd,stroke:#1565c0
```

**Extraction Categories**:
- **UserPreferences**: User likes, dislikes, requirements, communication style
- **Entities**: People, organizations, locations, technical concepts mentioned
- **Events**: Meetings, decisions, milestones, deadlines
- **Cases**: Specific problem-solving instances, troubleshooting steps
- **Facts**: Objective information, definitions, relationships

#### 5.1.3 Tiered Memory Layer Generation Flow

```mermaid
flowchart TB
    A[Agent Requests Context<br/>URI + Layer Level] --> B{Layer Cache<br/>Exists?}
    B -->|Yes| C[Return Cached Layer<br/>Filesystem Read]
    B -->|No| D[Load L2 Detail Layer<br/>Source Content]
    D --> E{Requested Level?}
    E -->|L0 Abstract| F[AbstractGenerator<br/>LLM: Summarize→1-2 sentences]
    E -->|L1 Overview| G[OverviewGenerator<br/>LLM: Structured bullets]
    F --> H[Cache to Filesystem<br/>TTL-based expiration]
    G --> H
    H --> I[Return Layer Content<br/>JSON/Markdown]
    C --> I
    I --> J[Deliver to Agent<br/>Context Window]
    
    style F fill:#e3f2fd,stroke:#1565c0
    style G fill:#e3f2fd,stroke:#1565c0
    style H fill:#fff3e0,stroke:#ef6c00
```

**Layer Strategy**:
- **L0 (Abstract)**: ~50 tokens; used for initial relevance filtering; high compression ratio
- **L1 (Overview)**: ~200-300 tokens; structured summary with key facts; medium compression
- **L2 (Detail)**: Full content; loaded only when specific details required; no compression

### 5.2 Technical Processing Workflows

#### 5.2.1 Session Lifecycle Management

```mermaid
flowchart LR
    A[Initiate Session] --> B[Create Session Context<br/>thread_id + metadata]
    B --> C[Message Accumulation<br/>User/Assistant/Role]
    C --> D[Persist to Timeline<br/>Markdown Format]
    D --> E[Queue for Indexing<br/>Async Vectorization]
    E --> F{Session Complete?}
    F -->|No| C
    F -->|Yes| G[Extract Memories<br/>LLM Analysis]
    G --> H[Update Session Metadata<br/>Stats/Summary]
    H --> I[Close Session<br/>Archive]
```

**Technical Details**:
- **Threading Model**: Sessions support hierarchical threading for branching conversations
- **Persistence Format**: Messages stored as Markdown with YAML frontmatter for metadata
- **Indexing Strategy**: Incremental indexing (new messages only) to avoid reprocessing history

#### 5.2.2 Memory Optimization and Maintenance

```mermaid
flowchart TD
    A[Scheduled Job Trigger] --> B[Optimization Engine<br/>Analyze Statistics]
    B --> C{Identify Issues}
    C -->|Duplicates| D[Vector Similarity Scan<br/>Threshold: 0.95+]
    C -->|Low Quality| E[LLM Quality Scoring<br/>Relevance/Coherence]
    C -->|Outdated| F[Temporal Analysis<br/>Last Access > Threshold]
    D --> G[Merge Duplicates<br/>Consolidate Metadata]
    E --> H[Archive or Remove<br/>Low-value Content]
    F --> I[Archive to Cold Storage]
    G --> J[Update Indexes]
    H --> J
    I --> J
    J --> K[Filesystem Cleanup<br/>Orphaned Files]
    K --> L[Update Dashboard<br/>Optimization Report]
```

### 5.3 Exception Handling Mechanisms

**Resilience Patterns**:
1. **LLM Fallback**: If primary LLM provider fails, system can fallback to secondary provider (configured in `config.toml`)
2. **Vector Store Degradation**: If Qdrant unavailable, system continues with filesystem-only operations (search degrades to metadata filtering)
3. **Embedding Retry**: Exponential backoff for embedding API rate limits; batch splitting for payload size errors
4. **Filesystem Locking**: File-based locking prevents concurrent writes to the same memory URI
5. **Circuit Breaker**: HTTP clients implement circuit breaker patterns for external service failures

---

## 6. Technical Implementation

### 6.1 Core Module Implementation Details

#### 6.1.1 Memory Type System

The domain model uses Rust's type system to enforce business rules at compile time:

```rust
// Core domain types (conceptual representation)
pub struct Memory {
    pub id: MemoryId,
    pub dimension: Dimension,      // User | Agent | Session
    pub category: Category,        // Memory | Profile | Entity | Event | Case
    pub content: Content,          // Text | Structured | Markdown
    pub metadata: Metadata,        // CreatedAt, UpdatedAt, Importance, Tags
    pub embedding_config: EmbeddingConfig,
}

pub enum Dimension {
    User(UserId),
    Agent(AgentId),
    Session(SessionId),
}

pub struct URI {
    scheme: String,                // "cortex"
    dimension: Dimension,
    category: Category,
    id: MemoryId,
}
// Maps to: cortex://user/{user_id}/memories/{memory_id}
```

**Design Rationale**:
- **Newtype Pattern**: `MemoryId`, `UserId` are newtype wrappers around UUIDs to prevent mixing ID types
- **Exhaustive Enums**: `Dimension` and `Category` enums ensure all cases are handled in match expressions
- **Zero-Copy Deserialization**: Uses `serde` with zero-copy string slices where possible for performance

#### 6.1.2 Vector Search Implementation

**Embedding Strategy**:
- **Model**: Default OpenAI `text-embedding-3-small` (1536 dimensions, high performance/cost ratio)
- **Batching**: Embeddings generated in batches of 100 for API efficiency
- **Chunking**: Large content (>8000 chars) is chunked with overlap (semantic chunking using paragraph boundaries)
- **Normalization**: Vectors L2-normalized before storage to enable cosine similarity via dot product

**Qdrant Schema**:
```json
{
  "collection_name": "cortex_memories",
  "vectors": {
    "size": 1536,
    "distance": "Cosine"
  },
  "payload_schema": {
    "dimension": "keyword",      // Enum: user, agent, session
    "scope_id": "keyword",       // User ID or Agent ID
    "category": "keyword",       // Memory category
    "created_at": "integer",     // Unix timestamp for range filtering
    "importance": "float"        // 0.0-1.0 for threshold filtering
  }
}
```

#### 6.1.3 Layer Generation Algorithm

**Caching Strategy**:
- **Cache Key**: SHA256 hash of source content + layer level + generation parameters
- **Storage**: Cached layers stored alongside source content: `{source_path}/layers/L0_{hash}.json`
- **Invalidation**: Content-based invalidation (if source changes, hash changes, new cache entry created; old entries cleaned by maintenance job)

**Prompt Engineering**:
- **L0 Abstract**: "Summarize the following content in 1-2 sentences focusing on key facts and decisions..."
- **L1 Overview**: "Extract key points as bullet points. Include: (1) Main topics, (2) Important facts, (3) Action items..."
- **Temperature**: 0.3 (deterministic) for abstracts, 0.7 (creative) for overviews

### 6.2 Key Algorithm Design

#### 6.2.1 Semantic Search with Metadata Filtering

```mermaid
flowchart LR
    A[查询部署问题] --> B[生成嵌入 text-embedding-3-small]
    B --> C[预过滤 元数据类别案例 范围用户ID123]
    C --> D[向量搜索 HNSW索引 Top-K100]
    D --> E[分数计算 相似度乘以重要性]
    E --> F[阈值过滤 分数大于075]
    F --> G[按分数排序 返回前10]
```

**Scoring Formula**:
```
Final Score = (Vector Similarity * 0.7) + (Importance Score * 0.2) + (Recency Boost * 0.1)
```
- **Recency Boost**: Exponential decay based on age (half-life: 30 days)

#### 6.2.2 Duplicate Detection

Uses vector similarity with clustering:
1. Calculate pairwise cosine similarity for memories within same dimension/scope
2. Hierarchical clustering with threshold 0.95 (nearly identical)
3. Merge clusters: Combine content, keep earliest timestamp, sum importance scores
4. Vector average for merged memory embedding

### 6.3 Data Structure Design

**Session Timeline Format** (Markdown with YAML frontmatter):
```markdown
---
session_id: "uuid"
created_at: 1699123456
participants: ["user:123", "agent:456"]
tags: ["deployment", "troubleshooting"]
---

## 2024-01-15 10:30:00 - user:123
Getting 502 errors when deploying to production.

## 2024-01-15 10:31:15 - agent:456
Let me check the logs. The issue appears to be database connection pooling...
```

**Memory JSON Schema**:
```json
{
  "id": "uuid",
  "dimension": "user",
  "scope_id": "user_123",
  "category": "preference",
  "content": {
    "type": "text",
    "body": "User prefers dark mode interfaces"
  },
  "metadata": {
    "created_at": 1699123456,
    "importance": 0.8,
    "source": "session_uuid",
    "tags": ["ui", "preference"]
  },
  "embedding": {
    "model": "text-embedding-3-small",
    "vector_id": "uuid"
  }
}
```

### 6.4 Performance Optimization Strategies

#### 6.4.1 Concurrency Model
- **Async/Await**: Tokio runtime with work-stealing scheduler
- **Connection Pooling**: HTTP connection pools for LLM and Qdrant clients (max 100 connections)
- **Backpressure**: Bounded channels (capacity 1000) for indexing queue to prevent memory exhaustion

#### 6.4.2 Caching Hierarchy
1. **L1 Cache**: In-memory LRU for frequently accessed memories (max 10,000 entries)
2. **L2 Cache**: Filesystem cache for generated layers (SSD-backed)
3. **L3 Storage**: Vector database (Qdrant) and primary filesystem

#### 6.4.3 Batch Processing
- **Embeddings**: Batch size 100 for API calls
- **Vector Upserts**: Batch size 500 for Qdrant
- **Filesystem Writes**: Buffered I/O with 4KB chunks

#### 6.4.4 Query Optimization
- **Projection**: Vector search returns only IDs + scores; full content loaded separately to reduce payload size
- **Pagination**: Cursor-based pagination for large result sets
- **Pre-filtering**: Metadata filters applied at Qdrant level before vector comparison (leverage payload indexes)

---

## 7. Deployment Architecture

### 7.1 Runtime Environment Requirements

**System Requirements**:
- **OS**: Linux (Ubuntu 20.04+), macOS (12+), Windows 10/11 (WSL2 recommended)
- **Rust**: Version 1.75+ (for core components)
- **Node.js**: Version 18+ (for Insights frontend)
- **Qdrant**: Version 1.7+ (vector database)
- **Network**: Outbound HTTPS access to LLM/Embedding APIs

**Resource Specifications** (Production):

| Component | CPU | Memory | Storage | Notes |
|-----------|-----|--------|---------|-------|
| cortex-mem-service | 2 cores | 512 MB | 1 GB | Stateless, horizontally scalable |
| cortex-mem-mcp | 1 core | 256 MB | 100 MB | Per-user process |
| Qdrant | 4 cores | 8 GB | 100 GB SSD | Memory-mapped indexes |
| cortex-mem-insights | 1 core | 512 MB | 500 MB | SSR SvelteKit |

### 7.2 Deployment Topology Structures

#### 7.2.1 Standalone/Developer Mode
```yaml
Architecture: Single binary
Components:
  - cortex-mem-cli (interactive)
Storage: Local filesystem + Embedded Qdrant (or local Docker)
LLM: Local inference (Ollama) or API keys
Use Case: Development, personal knowledge management
```

#### 7.2.2 Client-Server Mode
```yaml
Architecture: Distributed services
Components:
  - cortex-mem-service (centralized API)
  - cortex-mem-cli (remote client)
  - cortex-mem-insights (web dashboard)
Storage: 
  - Shared filesystem (NFS/S3)
  - Qdrant cluster (3+ nodes)
Load Balancing: Round-robin across service instances
Authentication: API key-based at service layer
Use Case: Team collaboration, multi-user agents
```

#### 7.2.3 MCP Integration Mode
```yaml
Architecture: Sidecar pattern
Components:
  - AI Agent (Claude Desktop, etc.)
  - cortex-mem-mcp (local process)
  - cortex-mem-core (embedded or remote)
Communication: stdio (MCP protocol) or SSE
Security: Process isolation, filesystem permissions
Use Case: AI assistant enhancement, desktop integration
```

### 7.3 Scalability Design

**Horizontal Scaling**:
- **cortex-mem-service**: Stateless design allows multiple instances behind load balancer; session affinity not required
- **Qdrant**: Supports distributed deployment with sharding; replicate across nodes for high availability
- **Filesystem**: Use shared storage (S3-compatible object storage or NFS) for multi-instance deployments

**Vertical Scaling**:
- **Memory**: Increase RAM for larger in-memory caches (L1 cache size configurable)
- **CPU**: Vector search is CPU-intensive; more cores improve concurrent search throughput
- **Network**: Bandwidth for LLM API calls (typically low volume due to caching)

**Data Partitioning Strategy**:
- **By Dimension**: Separate Qdrant collections for User/Agent/Session if dataset > 1M vectors per dimension
- **By Time**: Partition old sessions to cold storage (S3 Glacier) with metadata retained in database
- **By Tenant**: For SaaS deployments, separate data directories per tenant; consider separate Qdrant collections for tenant isolation

### 7.4 Monitoring and Operations

#### 7.4.1 Health Check Endpoints
- **cortex-mem-service**: `GET /health` returns 200 OK with component status (filesystem accessible, Qdrant reachable)
- **cortex-mem-mcp**: MCP protocol `ping` method
- **cortex-mem-insights**: Dashboard shows service health indicators

#### 7.4.2 Key Metrics (Exposed via Prometheus/OpenTelemetry)
| Metric | Type | Description |
|--------|------|-------------|
| `memory_operations_total` | Counter | Total CRUD operations |
| `search_latency_seconds` | Histogram | Vector search latency |
| `embedding_generation_seconds` | Histogram | LLM embedding API latency |
| `layer_cache_hit_ratio` | Gauge | L0/L1 cache hit rate |
| `filesystem_io_errors` | Counter | Storage operation failures |
| `qdrant_connection_pool_size` | Gauge | Active connections to vector DB |

#### 7.4.3 Alerting Rules
- **Critical**: Qdrant connection failure > 5 minutes
- **Warning**: Search latency p99 > 2 seconds
- **Info**: Cache hit ratio < 80% (suggests cache size misconfiguration)

#### 7.4.4 Backup and Disaster Recovery
- **Filesystem**: Regular snapshots of data directory (supports `rsync`, `restic`, or cloud-native backups)
- **Vector Database**: Qdrant supports snapshot API for point-in-time recovery
- **Configuration**: Version-controlled `config.toml` (exclude secrets)
- **Recovery Time Objective (RTO)**: < 1 hour (stateless services, data restored from backup)
- **Recovery Point Objective (RPO)**: < 5 minutes (continuous background indexing allows replay from filesystem)

---

## 8. Architectural Recommendations

### 8.1 Immediate Actions (Technical Debt)

1. **Standardize Insights Integration**: Migrate `cortex-mem-insights` server integrations to use REST API exclusively rather than direct core library linking to maintain architectural boundary integrity.

2. **Complete Rig Integration**: Finalize `cortex-mem-rig` tool definitions or merge into `cortex-mem-tools` to reduce crate sprawl and complete the Rig framework integration.

3. **API Gateway/BFF**: Consider adding a Backend-for-Frontend layer for the Insights dashboard to aggregate data from multiple core endpoints and prevent N+1 query issues.

### 8.2 Strategic Evolution

1. **Event Bus Architecture**: Introduce a message queue (Redis Streams or NATS) between Automation and Core domains to enable distributed processing of extraction jobs and better horizontal scaling.

2. **GraphQL API**: Consider adding GraphQL to `cortex-mem-service` to optimize Insights frontend data fetching, reduce over-fetching, and enable flexible client queries.

3. **WASM Compilation**: Compile `cortex-mem-core` to WebAssembly for potential browser-based memory operations in the Insights frontend, enabling offline capabilities.

4. **Federation Support**: Extend multi-tenancy to support federated deployments where different user groups can host their own Qdrant instances while using shared application services.

---

## Conclusion

Cortex-Mem demonstrates a **well-architected, domain-driven system** with clear separation of concerns and strong adherence to Clean Architecture principles. The Rust-based core provides type-safe, high-performance memory operations while the polyglot interface layer maximizes developer experience across different integration scenarios.

The architecture successfully implements **CQRS** for memory operations, **Event-Driven** automation for indexing, and **Layered Architecture** for progressive context loading. The identified gaps are minor integration standardization issues rather than fundamental design flaws.

**Architecture Grade**: A- (Excellent adherence to DDD and Clean Architecture, minor integration standardization needed)  
**Production Readiness**: High (Comprehensive error handling, observability, and deployment flexibility)

---

*Document generated based on comprehensive codebase analysis and domain relationship mapping. Confidence Level: 95%*