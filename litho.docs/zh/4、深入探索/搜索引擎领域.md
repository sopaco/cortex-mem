# 技术文档：搜索引擎领域

**模块:** `cortex-mem-core/src/search/vector_engine.rs`  
**领域:** 核心业务领域  
**版本:** 1.0  
**最后更新:** 2026-02-19 04:06:14 (UTC)

---

## 1. 执行概述

**搜索引擎领域**实现了一个智能的、多层语义搜索系统，支持跨Cortex-Mem平台的上下文感知记忆检索。它作为AI智能体记忆系统的主要检索机制，支持跨所有记忆维度（用户、智能体、会话和资源）的操作。

### 1.1 业务价值
- **精确检索**: 通过分层语义分析提供上下文相关的记忆
- **可扩展架构**: 通过渐进式精炼（L0→L1→L2）处理大型记忆语料库
- **自适应智能**: 根据查询意图和结果质量动态调整搜索策略
- **多租户支持**: 在跨租户保持搜索性能的同时强制执行数据隔离

---

## 2. 架构设计

### 2.1 高级架构

搜索引擎实现**三层检索模式**，带加权聚合，设计用于平衡计算效率和搜索精度：

```mermaid
graph TD
    A[用户查询] --> B[意图分析]
    B --> C{LLM可用?}
    C -->|是| D[查询重写]
    C -->|否| E[原始查询]
    D --> F[嵌入生成]
    E --> F
    F --> G[L0抽象搜索<br/>粗过滤]
    G --> H{找到结果?}
    H -->|否| I[降级策略<br/>阈值降低]
    H -->|是| J[L1概览搜索<br/>上下文精炼]
    I --> G
    J --> K[L2细节搜索<br/>精确匹配]
    K --> L[加权评分<br/>0.2L0+0.3L1+0.5L2]
    L --> M[排序和片段提取]
    M --> N[最终结果]
```

### 2.2 核心组件

| 组件 | 职责 | 技术栈 |
|-----------|------------------|------------------|
| **向量搜索引擎** | 编排分层检索和评分 | Rust, Async/Await, Tokio |
| **嵌入客户端** | 生成密集向量表示 | OpenAI兼容API |
| **Qdrant向量存储** | 持久化和相似性搜索 | Qdrant (gRPC/HTTP) |
| **意图检测器** | 分类查询类型用于自适应阈值 | 模式匹配、启发式 |
| **层级管理器** | 提供L0/L1摘要访问 | 文件系统 + LLM缓存 |
| **片段提取器** | 生成上下文文本窗口 | 字符串处理、正则 |

---

## 3. 三层检索模型

该系统实现受TARS/OpenViking记忆规范启发的**渐进披露**模式，跨三个抽象层级搜索：

### 3.1 层级定义

| 层级 | 内容类型 | 文件后缀 | 搜索目的 | 权重 |
|-------|--------------|-------------|----------------|--------|
| **L0（抽象）** | 高级摘要 | `.abstract.md` | 宽泛语义过滤 | 20% |
| **L1（概览）** | 结构化关键点 | `.overview.md` | 上下文精炼 | 30% |
| **L2（细节）** | 原始对话内容 | `.md`（完整） | 精确语义匹配 | 50% |

### 3.2 分层搜索策略

**阶段1: L0粗过滤**  
引擎首先使用自适应相似度阈值（默认0.5）搜索L0抽象。该层作为粗筛，识别与查询语义广泛匹配的可能目录，无需昂贵全文处理。

**阶段2: L1上下文精炼**  
对于有匹配L0内容的目录，引擎检索L1概览向量。该层提供结构化上下文（关键主题、实体、决策）精炼初始候选集。

**阶段3: L2精确匹配**  
在高分候选目录内，引擎对完整消息内容执行详细语义搜索。对于时间线目录，这涉及迭代单个消息；对于资源目录，直接内容比较。

**阶段4: 加权聚合**  
最终评分使用加权公式计算：
```
组合评分 = (0.2 × L0_评分) + (0.3 × L1_评分) + (0.5 × L2_评分)
```

---

## 4. 查询处理管道

### 4.1 意图检测和自适应阈值

引擎将传入查询分类为五个意图类别以优化检索参数：

| 意图类型 | 特征 | 自适应阈值 | 用例 |
|-------------|----------------|-------------------|----------|
| **事实** | "什么是..."、"何时..." | 0.4 | 实体查找、具体事实 |
| **搜索** | "查找..."、"寻找..." | 0.5 | 一般内容发现 |
| **关系** | "X与Y如何相关" | 0.5 | 连接分析 |
| **时间** | "上周..."、"昨天..." | 0.5 | 时间约束检索 |
| **一般** | 开放性问题 | 0.5 | 宽泛探索 |

**实现注意**: 阈值动态调整；事实查询使用更低阈值（0.4）以确保特定实体的召回，而一般查询保持更高精度（0.5）。

### 4.2 查询增强（可选）

当LLM客户端可用时，引擎执行**查询重写**以扩展或消歧用户输入：
- **同义词扩展**: 添加语义相关术语
- **上下文丰富**: 合并用户/智能体配置文件上下文
- **消歧**: 澄清模糊代词或引用

### 4.3 向量生成

查询文本使用配置的嵌入模型（默认：OpenAI兼容API）转换为密集嵌入。结果向量用作跨所有层相似度计算搜索键。

---

## 5. 健壮性：降级策略

引擎实现**优雅降级**以确保即使在非最佳条件也能交付结果：

### 5.1 阈值放松
如果L0搜索没有返回高于初始阈值的结果：
1. **首次重试**: 降低阈值0.5 → 0.4
2. **第二次重试**: 降低阈值0.4 → 0.3
3. **最终回退**: 跨L2层执行完整语义搜索（绕过L0/L1优化）

### 5.2 文件系统回退
对于未索引的L2内容（向量尚未生成），引擎回退到文件系统读取：
- 从`cortex://` URIs读取原始markdown内容
- 分配L2评分0.0（仅L0/L1加权贡献）
- 用于尚未同步到向量存储的实时内容

---

## 6. 结果处理

### 6.1 片段生成
对于每个合格结果，引擎提取上下文片段：
- **上下文窗口**: 匹配前后100个字符
- **最大长度**: 总共200个字符
- **高亮**: 查询词强调用于UI展示

### 6.2 元数据过滤
结果按以下条件后过滤：
- **租户隔离**: `tenant_id`严格匹配
- **时间范围**: `created_at`范围过滤
- **实体约束**: 特定参与者或维度过滤
- **URI前缀作用域**: 限制到特定记忆维度（用户/智能体/会话）的搜索

---

## 7. 集成接口

### 7.1 公共API表面

该模块暴露两个主要异步接口：

```rust
// 直接语义搜索（绕过层优化）
pub async fn semantic_search(
    &self,
    query: &str,
    options: SearchOptions
) -> Result<Vec<SearchResult>, SearchError>

// 带意图检测和降级的分层检索
pub async fn layered_semantic_search(
    &self,
    query: &str,
    options: SearchOptions
) -> Result<Vec<SearchResult>, SearchError>
```

### 7.2 依赖注入

引擎通过`Arc`维护到外部服务的线程安全引用：

```rust
pub struct VectorSearchEngine {
    vector_store: Arc<dyn QdrantVectorStore>,
    embedding_client: Arc<dyn EmbeddingClient>,
    filesystem: Arc<dyn CortexFilesystem>,
    llm_client: Option<Arc<dyn LLMClient>>, // 可选
}
```

### 7.3 跨领域交互

| 目标领域 | 交互模式 | 目的 |
|---------------|-------------------|---------|
| **层级管理** | 服务调用 | 检索L0/L1摘要 |
| **向量存储** | 服务调用 | 执行相似性搜索 |
| **核心基础设施** | 组合 | 访问文件系统和嵌入客户端 |
| **自动化管理** | 事件消费 | 重新索引已更新记忆 |

---

## 8. 配置和调优

### 8.1 搜索参数

| 参数 | 默认 | 描述 |
|-----------|--------|-------------|
| `limit` | 10 | 返回的最大结果数 |
| `threshold` | 自适应(0.4-0.5) | 最小相似度评分 |
| `scope` | 所有维度 | URI前缀过滤器（用户/智能体/会话） |
| `include_content` | false | 返回全文vs片段 |

### 8.2 性能优化

- **批量处理**: 目录内L2搜索批量处理以最小化向量存储往返
- **缓存**: L0/L1摘要文件系统缓存以避免重复LLM生成
- **租户隔离**: 集合后缀（`cortex-mem-{tenant_id}`）确保查询时无需过滤，提高性能

---

## 9. 实现细节

### 9.1 并发模型
- **Async/Await**: 所有I/O操作（向量搜索、文件系统、LLM）非阻塞
- **线程安全**: 依赖通过`Arc`共享，防止跨并发搜索请求的数据竞争
- **取消安全**: 长搜索尊重Tokio取消令牌

### 9.2 错误处理
- **格式错误的URI**: 带警告日志的优雅处理
- **缺失向量**: 回退到文件系统内容
- **LLM不可用**: 降级到原始查询（无重写）
- **向量存储超时**: 指数退避重试

### 9.3 余弦相似度
相似度评分使用归一化向量之间的余弦距离计算：
```
评分 = (query_vector · document_vector) / (||query|| × ||document||)
```

---

## 10. 使用示例

### 10.1 基本搜索
```rust
let results = engine.layered_semantic_search(
    "machine learning project requirements",
    SearchOptions {
        limit: 5,
        threshold: 0.5,
        scope: Some("cortex://session/"),
        ..Default::default()
    }
).await?;
```

### 10.2 事实查找（更低阈值）
```rust
let facts = engine.layered_semantic_search(
    "user's database password", // 检测到事实意图
    SearchOptions {
        threshold: 0.4, // 如果启用意图检测则自动调整
        limit: 3,
        ..Default::default()
    }
).await?;
```

---

## 11. 监控和可观察性

要监控的关键指标：
- **层级命中率**: L0→L1→L2进程比率
- **降级频率**: 回退策略触发频率
- **查询延迟**: 端到端搜索的P99百分位
- **向量存储延迟**: 每层Qdrant响应时间

---

## 12. 总结

搜索引擎领域代表了一个复杂的**分层检索系统**，在深度语义搜索的精度与粗过滤的性能之间取得平衡。通过利用L0/L1/L2抽象架构，它实现跨大型记忆语料库的亚秒搜索，同时保持AI智能体操作所需的上下文丰富性。

系统的**自适应阈值**和**降级策略**确保跨不同查询类型和系统状态的健壮性，使其适合需要高可用性和一致性能的生产部署。

**后续步骤**: 集成详情请参阅[会话管理领域]文档了解事件流，或[向量存储领域]文档了解底层持久化机制。
