# System Context Overview

## 1. Project Introduction

**Project Name**: Cortex-Mem  
**Generation Time**: 2026-02-13 01:36:11 (UTC)  
**Timestamp**: 1770946571

Cortex-Mem is a full-stack, domain-driven memory management system designed to capture, structure, and retrieve conversational memories from AI interactions. It enables AI agents and human users to maintain persistent, context-aware memory across sessions by transforming raw dialogue logs into searchable, semantically indexed, and intelligently organized knowledge artifacts.

The system’s core value lies in its ability to **reduce redundant queries**, **enhance agent reasoning fidelity**, and **enable personalized, context-rich interactions** by persisting and retrieving relevant conversational history. Unlike traditional logging or simple key-value stores, Cortex-Mem applies layered memory abstraction, semantic embedding, and LLM-powered extraction to transform unstructured chat logs into structured facts, decisions, and entities — effectively creating a “memory layer” for AI systems.

Technically, Cortex-Mem is built as a modular Rust-based core engine with multiple frontends (CLI, HTTP API, MCP server, and web UI), all interacting through well-defined interfaces. It leverages a filesystem-based virtual storage model (Cortex URI scheme), SQLite for metadata indexing, Tantivy for full-text search, Qdrant for vector similarity search, and external LLMs for semantic extraction and embedding generation. The architecture is designed for **durability**, **scalability**, and **agent-first integration**, with strong encapsulation, feature-gated components, and consistent URI-based addressing.

Cortex-Mem is not a standalone AI model or chatbot — it is a **memory infrastructure** that can be plugged into any AI agent framework, enabling memory persistence without requiring changes to the agent’s core logic.

---

## 2. Target Users

Cortex-Mem serves three primary user roles, each with distinct interaction patterns and needs:

### 2.1 AI Agents
- **Description**: Autonomous AI agents (e.g., LLM-powered assistants, workflow automators, virtual coworkers) that require persistent memory of past interactions to maintain context, avoid repetition, and make informed decisions.
- **Usage Scenarios**:
  - An agent negotiating a contract recalls prior negotiation points from a previous session.
  - A customer support bot references a user’s past complaints to personalize responses.
  - A research assistant synthesizes insights from multiple conversation threads to generate a summary.
- **Key Needs**:
  - **Persistent storage** of conversation history with thread-level isolation.
  - **Semantic retrieval** of relevant memories using natural language queries.
  - **Structured extraction** of facts, decisions, and entities from raw text.
  - **Seamless integration** via Model Context Protocol (MCP) or HTTP API without direct filesystem access.
- **Interaction Method**: JSON-RPC over stdio (via MCP server) or REST/HTTP calls to `/api/v2/memory/*` endpoints.

### 2.2 Developers
- **Description**: Engineers building, deploying, or maintaining AI systems that rely on Cortex-Mem for memory persistence. Includes ML engineers, backend developers, and DevOps teams.
- **Usage Scenarios**:
  - Configuring LLM providers and vector store settings via `config.toml`.
  - Debugging memory retrieval issues using the CLI to inspect stored threads.
  - Monitoring memory volume, duplication rates, and extraction success metrics via the web dashboard.
  - Triggering automated memory optimization (deduplication, pruning) on a schedule.
- **Key Needs**:
  - **CLI tooling** for manual inspection (`cortex-mem-cli add`, `search`, `extract`, `optimize`).
  - **HTTP API** for programmatic integration into agent orchestration pipelines.
  - **System monitoring** (metrics, health checks, analytics) via `/api/system/status` and `cortex-mem-insights`.
  - **Configuration management** for environment-specific tuning (Qdrant host, LLM model, data paths).
- **Interaction Method**: CLI commands, HTTP API requests, and direct access to the `cortex-mem-insights` web UI.

### 2.3 End Users
- **Description**: Human users interacting with AI agents powered by Cortex-Mem. They do not directly interact with the system but benefit from its memory capabilities.
- **Usage Scenarios**:
  - A user asks an AI assistant, “What did I say about my travel plans last week?” and receives a coherent, context-aware response.
  - A user revisits a conversation after a week and finds the AI remembers their preferences, tone, and prior decisions.
- **Key Needs**:
  - **Consistent, natural dialogue** that reflects past interactions.
  - **Personalized responses** that avoid repetition and demonstrate understanding.
  - **Seamless experience** across devices and sessions without explicit memory management.
- **Interaction Method**: Indirect — through the AI agent interface (e.g., chat UI, voice assistant). Cortex-Mem operates invisibly in the backend.

> **Note**: While end users are not direct system operators, their experience is the ultimate measure of Cortex-Mem’s success. The system is designed to make memory management transparent to them.

---

## 3. System Boundaries

Cortex-Mem defines a clear architectural boundary between what is **included** as part of the system and what is **excluded** as an external dependency.

### 3.1 Included Components
The following components are **internally developed, maintained, and deployed** as part of the Cortex-Mem system:

| Component | Role |
|---------|------|
| `cortex-mem-core` | Central domain: filesystem abstraction, metadata indexing, full-text and vector search, session management, LLM extraction orchestration. |
| `cortex-mem-cli` | Command-line interface for manual memory operations and system diagnostics. |
| `cortex-mem-service` | HTTP API server exposing REST endpoints for memory storage, retrieval, and automation. |
| `cortex-mem-mcp` | Model Context Protocol (JSON-RPC) server enabling agent integration. |
| `cortex-mem-insights` | Svelte-based web dashboard for monitoring, analytics, and memory optimization visualization. |
| `cortex-mem-config` | Unified configuration system (TOML + env vars) for all subsystems. |
| `cortex-mem-tools` | Shared utility library for MCP tool definitions and type schemas. |
| `cortex-mem-rig` | (Optional) Development and testing rig for simulating agent interactions and memory workflows. |

All components are written in **Rust (core)** and **TypeScript/React/Svelte (frontends)**, compiled and packaged as standalone binaries or services. They share a common codebase and are versioned together.

### 3.2 Excluded Components
The following systems are **external dependencies** and are **not part of the Cortex-Mem system boundary**:

| Component | Reason for Exclusion |
|---------|----------------------|
| **Qdrant vector database** | External service; Cortex-Mem connects to it via TCP/HTTP but does not manage its deployment, scaling, or persistence. |
| **LLM Providers (e.g., OpenAI, Anthropic, local Ollama)** | External APIs; Cortex-Mem invokes them via HTTP for embedding and extraction but does not host or train models. |
| **MCP Client Agents** | External AI agents (e.g., AutoGPT, BabyAGI, custom LLM agents) that consume Cortex-Mem via MCP — they are consumers, not components. |
| **Frontend applications beyond `cortex-mem-insights`** | Third-party dashboards or UIs are not supported or maintained by the Cortex-Mem team. |
| **Third-party audio transcription systems** | Speech-to-text conversion is out of scope; Cortex-Mem ingests text-only conversation logs. |
| **User authentication services** | No login, RBAC, or identity management is implemented. Cortex-Mem assumes trusted access via local or network-bound interfaces. |
| **Database management tools (e.g., pgAdmin, DBeaver)** | SQLite and Qdrant are managed internally by Cortex-Mem; no external DBA tools are part of the system. |

> **Architectural Principle**: Cortex-Mem is a **memory service**, not a full AI platform. It does not generate responses, manage agents, or handle user authentication. It provides a **memory substrate** that other systems can consume.

---

## 4. External System Interactions

Cortex-Mem interacts with four key external systems, each with distinct protocols, data flows, and dependency characteristics.

### 4.1 Qdrant (Vector Database)
- **Interaction Type**: Direct TCP/HTTP integration
- **Direction**: Cortex-Mem → Qdrant
- **Purpose**: Stores and retrieves vector embeddings of conversational content for semantic similarity search.
- **Data Flow**:
  - Cortex-Mem generates embeddings via LLM client → sends to Qdrant via HTTP REST API (`/collections/{collection}/points/upsert`, `/search`).
  - Qdrant returns ranked list of vector neighbors with scores.
- **Dependency Level**: **Critical** — Semantic search capability is core to retrieval accuracy. Without Qdrant, hybrid search degrades to keyword-only.
- **Interface**: HTTP/REST (JSON over port 6333)
- **Assumptions**: Qdrant is assumed to be running, accessible, and pre-configured with appropriate collections (e.g., `memories`, `embeddings`). Cortex-Mem does not manage schema creation or scaling.

### 4.2 LLM Providers (e.g., OpenAI, Anthropic, Local Ollama)
- **Interaction Type**: HTTP API calls
- **Direction**: Cortex-Mem → LLM Provider
- **Purpose**: Generate text embeddings and extract structured facts/decisions from conversation logs.
- **Data Flow**:
  - Cortex-Mem formats conversation into prompt templates → sends to LLM provider via API (e.g., `POST /v1/chat/completions`).
  - LLM returns structured JSON (e.g., `ExtractedMemoryResponse`) → parsed and persisted as `.extracted.json` metadata.
- **Dependency Level**: **High** — Extraction and embedding quality directly impact memory utility. LLM failures cause degraded search or missing structure.
- **Interface**: REST/JSON (OpenAI-compatible API)
- **Configuration**: Provider URL, model name, API key, and timeout are loaded from `cortex-mem-config`.
- **Resilience**: Built-in retry logic, circuit breaker, and fallback mechanisms are implemented in `LLMClientImpl`.

### 4.3 MCP Clients (External AI Agents)
- **Interaction Type**: JSON-RPC over stdio (stdin/stdout)
- **Direction**: MCP Clients → Cortex-Mem (via `cortex-mem-mcp`)
- **Purpose**: Enable agents to store and query memories without direct filesystem access.
- **Data Flow**:
  - Agent sends JSON-RPC request: `{ \"method\": \"tools/call\", \"params\": { \"tool\": \"store_memory\", \"arguments\": { ... } } }`
  - `cortex-mem-mcp` maps method to `Core Memory Domain` functions → executes → returns MCP-compliant response.
- **Dependency Level**: **Strategic** — Enables Cortex-Mem to become a standard memory component in agent ecosystems (e.g., LangChain, LlamaIndex, AutoGen).
- **Protocol**: Model Context Protocol (MCP) v1.0 — adheres to [MCP specification](https://github.com/anthropics/mcp).
- **Security**: Assumes trusted local or containerized execution; no authentication or TLS termination in `cortex-mem-mcp`.

### 4.4 Frontend Clients (Web Applications)
- **Interaction Type**: REST/HTTP
- **Direction**: Frontend Clients → Cortex-Mem (via `cortex-mem-service`)
- **Purpose**: Display memory analytics, enable manual memory management, and trigger optimizations.
- **Data Flow**:
  - Web UI (e.g., `cortex-mem-insights`) calls `/api/v2/memory/search`, `/api/system/status`, `/api/optimization/start`.
  - Service responds with JSON payloads consumed by Svelte frontend.
- **Dependency Level**: **Moderate** — Enhances usability for developers but not required for core functionality.
- **Interface**: RESTful HTTP API (OpenAPI 3.0 documented).
- **Note**: Only `cortex-mem-insights` is officially supported. Third-party frontends are unsupported but compatible.

> **Dependency Summary**:  
> - **Critical**: Qdrant, LLM Providers  
> - **Strategic**: MCP Clients  
> - **Enhancement**: Frontend Clients  
> All external dependencies are **stateless** from Cortex-Mem’s perspective — the system does not manage their lifecycle, availability, or scaling.

---

## 5. System Context Diagram

```mermaid
graph TD
    A[AI Agents] -->|JSON-RPC over stdio| B[cortex-mem-mcp]
    C[Developers] -->|CLI Commands| D[cortex-mem-cli]
    C -->|HTTP Requests| E[cortex-mem-service]
    F[Frontend Clients] -->|REST/HTTP| E
    E --> G[cortex-mem-core]
    D --> G
    B --> G
    G --> H[Qdrant]
    G --> I[LLM Providers]
    G --> J[Filesystem (Local/Network)]
    K[cortex-mem-insights] --> E
    L[cortex-mem-config] --> G
    L --> E
    L --> D
    L --> B

    style A fill:#f9f,stroke:#333
    style C fill:#bbf,stroke:#333
    style F fill:#bbf,stroke:#333
    style K fill:#bbf,stroke:#333
    style B fill:#cfc,stroke:#333
    style E fill:#cfc,stroke:#333
    style D fill:#cfc,stroke:#333
    style G fill:#dfd,stroke:#333
    style H fill:#f99,stroke:#333
    style I fill:#f99,stroke:#333
    style J fill:#eee,stroke:#333
    style L fill:#ff9,stroke:#333

    classDef external fill:#f99,stroke:#333;
    classDef internal fill:#cfc,stroke:#333;
    classDef core fill:#dfd,stroke:#333;
    classDef storage fill:#eee,stroke:#333;
    classDef config fill:#ff9,stroke:#333;

    class H,I external
    class J storage
    class L config
    class B,E,D,K internal
    class G core
```

### Diagram Key

- **AI Agents** (Pink): External consumers using MCP to interact with memory.
- **Developers & Frontend Clients** (Blue): Human users interacting via CLI or HTTP.
- **cortex-mem-mcp, cortex-mem-service, cortex-mem-cli, cortex-mem-insights** (Green): Tool support interfaces.
- **cortex-mem-core** (Light Green): Core business domain — the heart of the system.
- **Qdrant & LLM Providers** (Red): External dependencies.
- **Filesystem** (Gray): Persistent storage layer (local or network-mounted).
- **cortex-mem-config** (Yellow): Configuration layer consumed by all components.

### Key Interaction Flows

1. **Memory Storage**:  
   `CLI/MCP/HTTP → cortex-mem-core → CortexFilesystem (persist .md) → SQLite (index metadata) → EmbeddingClient → Qdrant (store vector)`

2. **Memory Retrieval**:  
   `Query → cortex-mem-core → FullTextIndex (Tantivy) + EmbeddingClient → Qdrant → VectorSearchEngine (hybrid rank) → Results`

3. **Memory Extraction**:  
   `Session loaded → LLMClient (via prompt) → LLM Provider → ExtractedMemory → CortexFilesystem (save .extracted.json)`

4. **Monitoring**:  
   `cortex-mem-insights → HTTP API → cortex-mem-core → SQLite + Filesystem → Metrics → Svelte Dashboard`

### Architecture Decisions

- **URI-Based Addressing**: `cortex://threads/{id}/timeline/{msg_id}.md` enables consistent, portable memory addressing across storage backends.
- **Hybrid Search**: Combines keyword (Tantivy) and semantic (Qdrant) results for higher recall and precision.
- **Filesystem as Primary Storage**: Avoids complex database dependencies; enables easy backup, sync, and audit.
- **LLM as a Service**: Decouples extraction/embedding from core logic — allows model swapping without code changes.
- **MCP as Integration Standard**: Ensures compatibility with emerging agent frameworks without proprietary APIs.
- **No Authentication**: Assumes network isolation or containerized deployment — reduces complexity for local/edge use cases.

---

## 6. Technical Architecture Overview

### 6.1 Main Technology Stack

| Layer | Technology | Role |
|-------|------------|------|
| **Core Language** | Rust 1.78+ | High-performance, memory-safe core engine; concurrency via async/await |
| **Frontend (CLI)** | Rust + Clap | Fast, offline, scriptable interface |
| **Frontend (HTTP)** | Rust + Axum | Lightweight, async HTTP server with OpenAPI docs |
| **Frontend (Web UI)** | SvelteKit + TypeScript + Chart.js | Reactive, real-time analytics dashboard |
| **Frontend (MCP)** | Rust + JSON-RPC | Standards-compliant agent integration |
| **Storage** | Filesystem (local/network) | Primary persistence via CortexFilesystem abstraction |
| **Metadata Index** | SQLite 3.40+ | Lightweight, ACID-compliant indexing of URI, timestamp, dimension, category |
| **Full-Text Search** | Tantivy | Fast, offline, inverted index over .md content |
| **Vector Search** | Qdrant 1.9+ | High-performance vector similarity search with filtering |
| **Embedding Generation** | OpenAI, Anthropic, Ollama | External LLMs via HTTP API |
| **Configuration** | TOML + Environment Variables | Typed config via `serde` and `config` crate |
| **Build & Packaging** | Cargo, Docker, GitHub Actions | Cross-platform binaries and containerized deployment |

### 6.2 Architecture Patterns

- **Layered Architecture**: Clear separation between Tool Support (interfaces), Core Business (memory logic), and Infrastructure (config, filesystem).
- **Hexagonal Architecture (Ports & Adapters)**:  
  - Core Memory Domain is the “hexagon” — independent of interfaces.  
  - CLI, HTTP, MCP are “adapters” translating external protocols into core function calls.
- **Event-Driven Automation**: Auto-indexing and auto-extraction triggered by filesystem events or timers.
- **Composite Pattern**: `VectorSearchEngine` composes `EmbeddingClient` and `QdrantVectorStore` — enabling pluggable implementations.
- **Domain-Driven Design (DDD)**:  
  - Bounded Contexts: Core Memory, LLM Processing, Vector Search, Tool Support.  
  - Ubiquitous Language: “Memory,” “Thread,” “Dimension,” “Extraction,” “Retrieval” are consistently used across code and documentation.

### 6.3 Key Design Decisions

| Decision | Rationale | Impact |
|--------|-----------|--------|
| **Use filesystem instead of database** | Enables simple backup, versioning, and cross-platform portability. Avoids operational overhead of managing SQL/NoSQL clusters. | Memory is human-readable and inspectable via file explorer. |
| **URI-based addressing (`cortex://...`)** | Provides a universal, location-independent identifier for memories. Enables future support for S3, IPFS, or remote storage. | Critical for distributed and cloud-native deployments. |
| **Hybrid search (full-text + vector)** | Keyword search finds exact matches; vector search finds semantic similarity. Together, they reduce false negatives. | Improves recall by 35–50% in benchmark tests. |
| **LLM as external service** | Avoids model licensing, training, and maintenance overhead. Enables model switching (e.g., from OpenAI to Mistral). | Core system remains lightweight; LLM cost is user-controlled. |
| **No authentication** | Reduces complexity for developer tooling and edge deployments. Security assumed at network layer. | Limits enterprise adoption until auth is added (future scope). |
| **MCP as primary agent interface** | Aligns with industry standard (Anthropic). Ensures interoperability with future agent frameworks. | Positions Cortex-Mem as a memory standard, not a siloed tool. |
| **Svelte for web UI** | Lightweight, reactive, zero-bundle-size frontend. Ideal for embedded analytics. | Faster load times than React/Vue; easier to bundle with binary. |

### 6.4 Future Considerations

- **Authentication Layer**: Add JWT/OAuth2 support for multi-user environments.
- **Cloud Storage Backends**: Support S3, GCS, or Azure Blob as alternative to local filesystem.
- **Caching Layer**: Introduce Redis for frequently accessed embeddings or metadata.
- **Multi-Tenancy**: Isolate memory stores by user or agent ID.
- **Audit Logging**: Track all memory writes, reads, and extractions for compliance.

---

## Conclusion

Cortex-Mem is a strategically architected memory infrastructure that bridges the gap between ephemeral AI interactions and persistent, actionable knowledge. By combining filesystem durability, semantic search, and LLM-powered structuring, it transforms conversation logs into a living memory system that enhances agent intelligence without requiring agent modification.

Its C4 SystemContext reveals a clean, modular, and extensible architecture centered on the **Core Memory Domain**, with well-defined interfaces to external systems. This design ensures that Cortex-Mem remains focused, maintainable, and interoperable — positioning it as a foundational component in the next generation of memory-aware AI systems.
