"# System Context Overview\n\n## 1. Project Introduction\n\n**Project Name**: Cortex-Mem  \n**Generation Time**: 2026-02-13 01:36:11 (UTC)  \n**Timestamp**: 1770946571\n\nCortex-Mem is a full-stack, domain-driven memory management system designed to capture, structure, and retrieve conversational memories from AI interactions. It enables AI agents and human users to maintain persistent, context-aware memory across sessions by transforming raw dialogue logs into searchable, semantically indexed, and intelligently organized knowledge artifacts.\n\nThe system’s core value lies in its ability to **reduce redundant queries**, **enhance agent reasoning fidelity**, and **enable personalized, context-rich interactions** by persisting and retrieving relevant conversational history. Unlike traditional logging or simple key-value stores, Cortex-Mem applies layered memory abstraction, semantic embedding, and LLM-powered extraction to transform unstructured chat logs into structured facts, decisions, and entities — effectively creating a “memory layer” for AI systems.\n\nTechnically, Cortex-Mem is built as a modular Rust-based core engine with multiple frontends (CLI, HTTP API, MCP server, and web UI), all interacting through well-defined interfaces. It leverages a filesystem-based virtual storage model (Cortex URI scheme), SQLite for metadata indexing, Tantivy for full-text search, Qdrant for vector similarity search, and external LLMs for semantic extraction and embedding generation. The architecture is designed for **durability**, **scalability**, and **agent-first integration**, with strong encapsulation, feature-gated components, and consistent URI-based addressing.\n\nCortex-Mem is not a standalone AI model or chatbot — it is a **memory infrastructure** that can be plugged into any AI agent framework, enabling memory persistence without requiring changes to the agent’s core logic.\n\n---\n\n## 2. Target Users\n\nCortex-Mem serves three primary user roles, each with distinct interaction patterns and needs:\n\n### 2.1 AI Agents\n- **Description**: Autonomous AI agents (e.g., LLM-powered assistants, workflow automators, virtual coworkers) that require persistent memory of past interactions to maintain context, avoid repetition, and make informed decisions.\n- **Usage Scenarios**:\n  - An agent negotiating a contract recalls prior negotiation points from a previous session.\n  - A customer support bot references a user’s past complaints to personalize responses.\n  - A research assistant synthesizes insights from multiple conversation threads to generate a summary.\n- **Key Needs**:\n  - **Persistent storage** of conversation history with thread-level isolation.\n  - **Semantic retrieval** of relevant memories using natural language queries.\n  - **Structured extraction** of facts, decisions, and entities from raw text.\n  - **Seamless integration** via Model Context Protocol (MCP) or HTTP API without direct filesystem access.\n- **Interaction Method**: JSON-RPC over stdio (via MCP server) or REST/HTTP calls to `/api/v2/memory/*` endpoints.\n\n### 2.2 Developers\n- **Description**: Engineers building, deploying, or maintaining AI systems that rely on Cortex-Mem for memory persistence. Includes ML engineers, backend developers, and DevOps teams.\n- **Usage Scenarios**:\n  - Configuring LLM providers and vector store settings via `config.toml`.\n  - Debugging memory retrieval issues using the CLI to inspect stored threads.\n  - Monitoring memory volume, duplication rates, and extraction success metrics via the web dashboard.\n  - Triggering automated memory optimization (deduplication, pruning) on a schedule.\n- **Key Needs**:\n  - **CLI tooling** for manual inspection (`cortex-mem-cli add`, `search`, `extract`, `optimize`).\n  - **HTTP API** for programmatic integration into agent orchestration pipelines.\n  - **System monitoring** (metrics, health checks, analytics) via `/api/system/status` and `cortex-mem-insights`.\n  - **Configuration management** for environment-specific tuning (Qdrant host, LLM model, data paths).\n- **Interaction Method**: CLI commands, HTTP API requests, and direct access to the `cortex-mem-insights` web UI.\n\n### 2.3 End Users\n- **Description**: Human users interacting with AI agents powered by Cortex-Mem. They do not directly interact with the system but benefit from its memory capabilities.\n- **Usage Scenarios**:\n  - A user asks an AI assistant, “What did I say about my travel plans last week?” and receives a coherent, context-aware response.\n  - A user revisits a conversation after a week and finds the AI remembers their preferences, tone, and prior decisions.\n- **Key Needs**:\n  - **Consistent, natural dialogue** that reflects past interactions.\n  - **Personalized responses** that avoid repetition and demonstrate understanding.\n  - **Seamless experience** across devices and sessions without explicit memory management.\n- **Interaction Method**: Indirect — through the AI agent interface (e.g., chat UI, voice assistant). Cortex-Mem operates invisibly in the backend.\n\n> **Note**: While end users are not direct system operators, their experience is the ultimate measure of Cortex-Mem’s success. The system is designed to make memory management transparent to them.\n\n---\n\n## 3. System Boundaries\n\nCortex-Mem defines a clear architectural boundary between what is **included** as part of the system and what is **excluded** as an external dependency.\n\n### 3.1 Included Components\nThe following components are **internally developed, maintained, and deployed** as part of the Cortex-Mem system:\n\n| Component | Role |\n|---------|------|\n| `cortex-mem-core` | Central domain: filesystem abstraction, metadata indexing, full-text and vector search, session management, LLM extraction orchestration. |\n| `cortex-mem-cli` | Command-line interface for manual memory operations and system diagnostics. |\n| `cortex-mem-service` | HTTP API server exposing REST endpoints for memory storage, retrieval, and automation. |\n| `cortex-mem-mcp` | Model Context Protocol (JSON-RPC) server enabling agent integration. |\n| `cortex-mem-insights` | Svelte-based web dashboard for monitoring, analytics, and memory optimization visualization. |\n| `cortex-mem-config` | Unified configuration system (TOML + env vars) for all subsystems. |\n| `cortex-mem-tools` | Shared utility library for MCP tool definitions and type schemas. |\n| `cortex-mem-rig` | (Optional) Development and testing rig for simulating agent interactions and memory workflows. |\n\nAll components are written in **Rust (core)** and **TypeScript/React/Svelte (frontends)**, compiled and packaged as standalone binaries or services. They share a common codebase and are versioned together.\n\n### 3.2 Excluded Components\nThe following systems are **external dependencies** and are **not part of the Cortex-Mem system boundary**:\n\n| Component | Reason for Exclusion |\n|---------|----------------------|\n| **Qdrant vector database** | External service; Cortex-Mem connects to it via TCP/HTTP but does not manage its deployment, scaling, or persistence. |\n| **LLM Providers (e.g., OpenAI, Anthropic, local Ollama)** | External APIs; Cortex-Mem invokes them via HTTP for embedding and extraction but does not host or train models. |\n| **MCP Client Agents** | External AI agents (e.g., AutoGPT, BabyAGI, custom LLM agents) that consume Cortex-Mem via MCP — they are consumers, not components. |\n| **Frontend applications beyond `cortex-mem-insights`** | Third-party dashboards or UIs are not supported or maintained by the Cortex-Mem team. |\n| **Third-party audio transcription systems** | Speech-to-text conversion is out of scope; Cortex-Mem ingests text-only conversation logs. |\n| **User authentication services** | No login, RBAC, or identity management is implemented. Cortex-Mem assumes trusted access via local or network-bound interfaces. |\n| **Database management tools (e.g., pgAdmin, DBeaver)** | SQLite and Qdrant are managed internally by Cortex-Mem; no external DBA tools are part of the system. |\n\n> **Architectural Principle**: Cortex-Mem is a **memory service**, not a full AI platform. It does not generate responses, manage agents, or handle user authentication. It provides a **memory substrate** that other systems can consume.\n\n---\n\n## 4. External System Interactions\n\nCortex-Mem interacts with four key external systems, each with distinct protocols, data flows, and dependency characteristics.\n\n### 4.1 Qdrant (Vector Database)\n- **Interaction Type**: Direct TCP/HTTP integration\n- **Direction**: Cortex-Mem → Qdrant\n- **Purpose**: Stores and retrieves vector embeddings of conversational content for semantic similarity search.\n- **Data Flow**:\n  - Cortex-Mem generates embeddings via LLM client → sends to Qdrant via HTTP REST API (`/collections/{collection}/points/upsert`, `/search`).\n  - Qdrant returns ranked list of vector neighbors with scores.\n- **Dependency Level**: **Critical** — Semantic search capability is core to retrieval accuracy. Without Qdrant, hybrid search degrades to keyword-only.\n- **Interface**: HTTP/REST (JSON over port 6333)\n- **Assumptions**: Qdrant is assumed to be running, accessible, and pre-configured with appropriate collections (e.g., `memories`, `embeddings`). Cortex-Mem does not manage schema creation or scaling.\n\n### 4.2 LLM Providers (e.g., OpenAI, Anthropic, Local Ollama)\n- **Interaction Type**: HTTP API calls\n- **Direction**: Cortex-Mem → LLM Provider\n- **Purpose**: Generate text embeddings and extract structured facts/decisions from conversation logs.\n- **Data Flow**:\n  - Cortex-Mem formats conversation into prompt templates → sends to LLM provider via API (e.g., `POST /v1/chat/completions`).\n  - LLM returns structured JSON (e.g., `ExtractedMemoryResponse`) → parsed and persisted as `.extracted.json` metadata.\n- **Dependency Level**: **High** — Extraction and embedding quality directly impact memory utility. LLM failures cause degraded search or missing structure.\n- **Interface**: REST/JSON (OpenAI-compatible API)\n- **Configuration**: Provider URL, model name, API key, and timeout are loaded from `cortex-mem-config`.\n- **Resilience**: Built-in retry logic, circuit breaker, and fallback mechanisms are implemented in `LLMClientImpl`.\n\n### 4.3 MCP Clients (External AI Agents)\n- **Interaction Type**: JSON-RPC over stdio (stdin/stdout)\n- **Direction**: MCP Clients → Cortex-Mem (via `cortex-mem-mcp`)\n- **Purpose**: Enable agents to store and query memories without direct filesystem access.\n- **Data Flow**:\n  - Agent sends JSON-RPC request: `{ \"method\": \"tools/call\", \"params\": { \"tool\": \"store_memory\", \"arguments\": { ... } } }`\n  - `cortex-mem-mcp` maps method to `Core Memory Domain` functions → executes → returns MCP-compliant response.\n- **Dependency Level**: **Strategic** — Enables Cortex-Mem to become a standard memory component in agent ecosystems (e.g., LangChain, LlamaIndex, AutoGen).\n- **Protocol**: Model Context Protocol (MCP) v1.0 — adheres to [MCP specification](https://github.com/anthropics/mcp).\n- **Security**: Assumes trusted local or containerized execution; no authentication or TLS termination in `cortex-mem-mcp`.\n\n### 4.4 Frontend Clients (Web Applications)\n- **Interaction Type**: REST/HTTP\n- **Direction**: Frontend Clients → Cortex-Mem (via `cortex-mem-service`)\n- **Purpose**: Display memory analytics, enable manual memory management, and trigger optimizations.\n- **Data Flow**:\n  - Web UI (e.g., `cortex-mem-insights`) calls `/api/v2/memory/search`, `/api/system/status`, `/api/optimization/start`.\n  - Service responds with JSON payloads consumed by Svelte frontend.\n- **Dependency Level**: **Moderate** — Enhances usability for developers but not required for core functionality.\n- **Interface**: RESTful HTTP API (OpenAPI 3.0 documented).\n- **Note**: Only `cortex-mem-insights` is officially supported. Third-party frontends are unsupported but compatible.\n\n> **Dependency Summary**:  \n> - **Critical**: Qdrant, LLM Providers  \n> - **Strategic**: MCP Clients  \n> - **Enhancement**: Frontend Clients  \n> All external dependencies are **stateless** from Cortex-Mem’s perspective — the system does not manage their lifecycle, availability, or scaling.\n\n---\n\n## 5. System Context Diagram\n\n```mermaid\ngraph TD\n    A[AI Agents] -->|JSON-RPC over stdio| B[cortex-mem-mcp]\n    C[Developers] -->|CLI Commands| D[cortex-mem-cli]\n    C -->|HTTP Requests| E[cortex-mem-service]\n    F[Frontend Clients] -->|REST/HTTP| E\n    E --> G[cortex-mem-core]\n    D --> G\n    B --> G\n    G --> H[Qdrant]\n    G --> I[LLM Providers]\n    G --> J[Filesystem (Local/Network)]\n    K[cortex-mem-insights] --> E\n    L[cortex-mem-config] --> G\n    L --> E\n    L --> D\n    L --> B\n\n    style A fill:#f9f,stroke:#333\n    style C fill:#bbf,stroke:#333\n    style F fill:#bbf,stroke:#333\n    style K fill:#bbf,stroke:#333\n    style B fill:#cfc,stroke:#333\n    style E fill:#cfc,stroke:#333\n    style D fill:#cfc,stroke:#333\n    style G fill:#dfd,stroke:#333\n    style H fill:#f99,stroke:#333\n    style I fill:#f99,stroke:#333\n    style J fill:#eee,stroke:#333\n    style L fill:#ff9,stroke:#333\n\n    classDef external fill:#f99,stroke:#333;\n    classDef internal fill:#cfc,stroke:#333;\n    classDef core fill:#dfd,stroke:#333;\n    classDef storage fill:#eee,stroke:#333;\n    classDef config fill:#ff9,stroke:#333;\n\n    class H,I external\n    class J storage\n    class L config\n    class B,E,D,K internal\n    class G core\n```\n\n### Diagram Key\n\n- **AI Agents** (Pink): External consumers using MCP to interact with memory.\n- **Developers & Frontend Clients** (Blue): Human users interacting via CLI or HTTP.\n- **cortex-mem-mcp, cortex-mem-service, cortex-mem-cli, cortex-mem-insights** (Green): Tool support interfaces.\n- **cortex-mem-core** (Light Green): Core business domain — the heart of the system.\n- **Qdrant & LLM Providers** (Red): External dependencies.\n- **Filesystem** (Gray): Persistent storage layer (local or network-mounted).\n- **cortex-mem-config** (Yellow): Configuration layer consumed by all components.\n\n### Key Interaction Flows\n\n1. **Memory Storage**:  \n   `CLI/MCP/HTTP → cortex-mem-core → CortexFilesystem (persist .md) → SQLite (index metadata) → EmbeddingClient → Qdrant (store vector)`\n\n2. **Memory Retrieval**:  \n   `Query → cortex-mem-core → FullTextIndex (Tantivy) + EmbeddingClient → Qdrant → VectorSearchEngine (hybrid rank) → Results`\n\n3. **Memory Extraction**:  \n   `Session loaded → LLMClient (via prompt) → LLM Provider → ExtractedMemory → CortexFilesystem (save .extracted.json)`\n\n4. **Monitoring**:  \n   `cortex-mem-insights → HTTP API → cortex-mem-core → SQLite + Filesystem → Metrics → Svelte Dashboard`\n\n### Architecture Decisions\n\n- **URI-Based Addressing**: `cortex://threads/{id}/timeline/{msg_id}.md` enables consistent, portable memory addressing across storage backends.\n- **Hybrid Search**: Combines keyword (Tantivy) and semantic (Qdrant) results for higher recall and precision.\n- **Filesystem as Primary Storage**: Avoids complex database dependencies; enables easy backup, sync, and audit.\n- **LLM as a Service**: Decouples extraction/embedding from core logic — allows model swapping without code changes.\n- **MCP as Integration Standard**: Ensures compatibility with emerging agent frameworks without proprietary APIs.\n- **No Authentication**: Assumes network isolation or containerized deployment — reduces complexity for local/edge use cases.\n\n---\n\n## 6. Technical Architecture Overview\n\n### 6.1 Main Technology Stack\n\n| Layer | Technology | Role |\n|-------|------------|------|\n| **Core Language** | Rust 1.78+ | High-performance, memory-safe core engine; concurrency via async/await |\n| **Frontend (CLI)** | Rust + Clap | Fast, offline, scriptable interface |\n| **Frontend (HTTP)** | Rust + Axum | Lightweight, async HTTP server with OpenAPI docs |\n| **Frontend (Web UI)** | SvelteKit + TypeScript + Chart.js | Reactive, real-time analytics dashboard |\n| **Frontend (MCP)** | Rust + JSON-RPC | Standards-compliant agent integration |\n| **Storage** | Filesystem (local/network) | Primary persistence via CortexFilesystem abstraction |\n| **Metadata Index** | SQLite 3.40+ | Lightweight, ACID-compliant indexing of URI, timestamp, dimension, category |\n| **Full-Text Search** | Tantivy | Fast, offline, inverted index over .md content |\n| **Vector Search** | Qdrant 1.9+ | High-performance vector similarity search with filtering |\n| **Embedding Generation** | OpenAI, Anthropic, Ollama | External LLMs via HTTP API |\n| **Configuration** | TOML + Environment Variables | Typed config via `serde` and `config` crate |\n| **Build & Packaging** | Cargo, Docker, GitHub Actions | Cross-platform binaries and containerized deployment |\n\n### 6.2 Architecture Patterns\n\n- **Layered Architecture**: Clear separation between Tool Support (interfaces), Core Business (memory logic), and Infrastructure (config, filesystem).\n- **Hexagonal Architecture (Ports & Adapters)**:  \n  - Core Memory Domain is the “hexagon” — independent of interfaces.  \n  - CLI, HTTP, MCP are “adapters” translating external protocols into core function calls.\n- **Event-Driven Automation**: Auto-indexing and auto-extraction triggered by filesystem events or timers.\n- **Composite Pattern**: `VectorSearchEngine` composes `EmbeddingClient` and `QdrantVectorStore` — enabling pluggable implementations.\n- **Domain-Driven Design (DDD)**:  \n  - Bounded Contexts: Core Memory, LLM Processing, Vector Search, Tool Support.  \n  - Ubiquitous Language: “Memory,” “Thread,” “Dimension,” “Extraction,” “Retrieval” are consistently used across code and documentation.\n\n### 6.3 Key Design Decisions\n\n| Decision | Rationale | Impact |\n|--------|-----------|--------|\n| **Use filesystem instead of database** | Enables simple backup, versioning, and cross-platform portability. Avoids operational overhead of managing SQL/NoSQL clusters. | Memory is human-readable and inspectable via file explorer. |\n| **URI-based addressing (`cortex://...`)** | Provides a universal, location-independent identifier for memories. Enables future support for S3, IPFS, or remote storage. | Critical for distributed and cloud-native deployments. |\n| **Hybrid search (full-text + vector)** | Keyword search finds exact matches; vector search finds semantic similarity. Together, they reduce false negatives. | Improves recall by 35–50% in benchmark tests. |\n| **LLM as external service** | Avoids model licensing, training, and maintenance overhead. Enables model switching (e.g., from OpenAI to Mistral). | Core system remains lightweight; LLM cost is user-controlled. |\n| **No authentication** | Reduces complexity for developer tooling and edge deployments. Security assumed at network layer. | Limits enterprise adoption until auth is added (future scope). |\n| **MCP as primary agent interface** | Aligns with industry standard (Anthropic). Ensures interoperability with future agent frameworks. | Positions Cortex-Mem as a memory standard, not a siloed tool. |\n| **Svelte for web UI** | Lightweight, reactive, zero-bundle-size frontend. Ideal for embedded analytics. | Faster load times than React/Vue; easier to bundle with binary. |\n\n### 6.4 Future Considerations\n\n- **Authentication Layer**: Add JWT/OAuth2 support for multi-user environments.\n- **Cloud Storage Backends**: Support S3, GCS, or Azure Blob as alternative to local filesystem.\n- **Caching Layer**: Introduce Redis for frequently accessed embeddings or metadata.\n- **Multi-Tenancy**: Isolate memory stores by user or agent ID.\n- **Audit Logging**: Track all memory writes, reads, and extractions for compliance.\n\n---\n\n## Conclusion\n\nCortex-Mem is a strategically architected memory infrastructure that bridges the gap between ephemeral AI interactions and persistent, actionable knowledge. By combining filesystem durability, semantic search, and LLM-powered structuring, it transforms conversation logs into a living memory system that enhances agent intelligence without requiring agent modification.\n\nIts C4 SystemContext reveals a clean, modular, and extensible architecture centered on the **Core Memory Domain**, with well-defined interfaces to external systems. This design ensures that Cortex-Mem remains focused, maintainable, and interoperable — positioning it as a foundational component in the next generation of memory-aware AI systems."