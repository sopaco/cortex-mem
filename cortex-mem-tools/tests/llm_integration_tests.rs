//! Cortex-Mem LLM é›†æˆæµ‹è¯•
//!
//! è¿™äº›æµ‹è¯•éœ€è¦æœ‰æ•ˆçš„ LLM é…ç½®æ‰èƒ½è¿è¡Œ
//! é…ç½®æ¥æºï¼šconfig.toml æˆ–ç¯å¢ƒå˜é‡

use cortex_mem_tools::{MemoryOperations, types::*};
use std::sync::Arc;
use tempfile::TempDir;

/// æ£€æŸ¥æ˜¯å¦æœ‰ LLM é…ç½®
fn has_llm_config() -> bool {
    // å…ˆå°è¯•ä» config.toml åŠ è½½ï¼ˆä»å¤šä¸ªå¯èƒ½çš„ä½ç½®æŸ¥æ‰¾ï¼‰
    if load_llm_config_from_file().is_some() {
        return true;
    }
    
    // æˆ–è€…æ£€æŸ¥ç¯å¢ƒå˜é‡
    std::env::var("LLM_API_BASE_URL").is_ok() && 
    std::env::var("LLM_API_KEY").is_ok()
}

/// ä» config.toml è§£æ LLM é…ç½®
fn load_llm_config_from_file() -> Option<cortex_mem_core::llm::LLMConfig> {
    // å°è¯•ä»å¤šä¸ªä½ç½®æŸ¥æ‰¾ config.toml
    let possible_paths = [
        "config.toml",  // å½“å‰ç›®å½•
        "../config.toml",  // ä¸Šçº§ç›®å½•ï¼ˆä» cortex-mem-tools è¿è¡Œæ—¶ï¼‰
        "../../config.toml",  // ä¸Šä¸¤çº§ç›®å½•
    ];
    
    let mut content = None;
    let mut found_path = "";
    
    for path in &possible_paths {
        if let Ok(c) = std::fs::read_to_string(path) {
            content = Some(c);
            found_path = path;
            break;
        }
    }
    
    let content = content?;
    
    // æ£€æŸ¥æ˜¯å¦æœ‰ [llm] æ®µè½
    if !content.contains("[llm]") {
        println!("âš ï¸ config.toml ä¸­æ²¡æœ‰ [llm] é…ç½®æ®µè½");
        return None;
    }
    
    // ç®€å•è§£æ TOML
    let mut api_base_url = None;
    let mut api_key = None;
    let mut model = Some("gpt-3.5-turbo".to_string());
    let mut temperature = Some(0.1f32);
    let mut max_tokens = Some(4096u32);
    
    let mut in_llm_section = false;
    for line in content.lines() {
        let trimmed = line.trim();
        
        // è·³è¿‡ç©ºè¡Œ
        if trimmed.is_empty() {
            continue;
        }
        
        // æ£€æµ‹ [llm] æ®µè½å¼€å§‹
        if trimmed == "[llm]" {
            in_llm_section = true;
            continue;
        }
        
        // æ£€æµ‹å…¶ä»–æ®µè½å¼€å§‹ï¼ˆç»“æŸ [llm] æ®µè½ï¼‰
        if trimmed.starts_with('[') && in_llm_section {
            break;
        }
        
        // åœ¨ [llm] æ®µè½å†…
        if in_llm_section {
            // è·³è¿‡æ³¨é‡Šè¡Œï¼ˆä»¥ # å¼€å¤´ï¼‰
            if trimmed.starts_with('#') {
                continue;
            }
            
            // è§£æ key = "value" æ ¼å¼
            if let Some(eq_pos) = trimmed.find('=') {
                let key = trimmed[..eq_pos].trim();
                let value_part = trimmed[eq_pos + 1..].trim();
                
                // è·³è¿‡æ³¨é‡Šæ‰çš„é…ç½®ï¼ˆkey ä»¥ # å¼€å¤´ï¼‰
                if key.starts_with('#') {
                    continue;
                }
                
                // ç§»é™¤å¼•å·
                let value = value_part
                    .trim_matches('"')
                    .trim_matches('\'')
                    .to_string();
                
                match key {
                    "api_base_url" => api_base_url = Some(value),
                    "api_key" => api_key = Some(value),
                    "model_efficient" | "model" => model = Some(value),
                    "temperature" => temperature = value.parse().ok(),
                    "max_tokens" => max_tokens = value.parse().ok(),
                    _ => {}
                }
            }
        }
    }
    
    // æ£€æŸ¥æ˜¯å¦è·å–äº†å¿…éœ€çš„é…ç½®
    let api_url = api_base_url?;
    let key = api_key?;
    
    // æ£€æŸ¥å€¼æ˜¯å¦ä¸ºç©º
    if api_url.is_empty() || key.is_empty() {
        println!("âš ï¸ config.toml ä¸­çš„ api_base_url æˆ– api_key ä¸ºç©º");
        return None;
    }
    
    Some(cortex_mem_core::llm::LLMConfig {
        api_base_url: api_url,
        api_key: key,
        model_efficient: model?,
        temperature: temperature?,
        max_tokens: max_tokens? as usize,
    })
}

/// åŠ è½½ LLM é…ç½®
fn load_llm_config() -> Option<cortex_mem_core::llm::LLMConfig> {
    // ä¼˜å…ˆä» config.toml åŠ è½½
    if let Some(config) = load_llm_config_from_file() {
        return Some(config);
    }
    
    // ä»ç¯å¢ƒå˜é‡åŠ è½½
    if let (Ok(api_url), Ok(api_key)) = (
        std::env::var("LLM_API_BASE_URL"),
        std::env::var("LLM_API_KEY"),
    ) {
        return Some(cortex_mem_core::llm::LLMConfig {
            api_base_url: api_url,
            api_key,
            model_efficient: std::env::var("LLM_MODEL").unwrap_or_else(|_| "gpt-3.5-turbo".to_string()),
            temperature: 0.1,
            max_tokens: 4096,
        });
    }
    
    None
}

/// åˆ›å»ºå¸¦ LLM çš„æµ‹è¯•ç¯å¢ƒ
async fn setup_llm_test_env() -> Option<(TempDir, MemoryOperations)> {
    if !has_llm_config() {
        return None;
    }
    
    let llm_config = load_llm_config()?;
    let llm_client = Arc::new(
        cortex_mem_core::llm::LLMClientImpl::new(llm_config).ok()?
    );
    
    let temp_dir = TempDir::new().unwrap();
    let ops = MemoryOperations::with_tenant_and_llm(
        temp_dir.path().to_str().unwrap(),
        "llm_test_tenant",
        llm_client,
    ).await.ok()?;
    
    Some((temp_dir, ops))
}

// ==================== LLM åŠŸèƒ½æµ‹è¯• ====================

mod llm_layer_tests {
    use super::*;

    /// æµ‹è¯• LLM ç”Ÿæˆçš„é«˜è´¨é‡ L0 æ‘˜è¦
    #[tokio::test]
    async fn test_llm_l0_quality() {
        let env = setup_llm_test_env().await;
        if env.is_none() {
            println!("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæ²¡æœ‰ LLM é…ç½®");
            return;
        }
        
        let (_temp_dir, ops) = env.unwrap();
        
        // ä½¿ç”¨éœ€è¦ç†è§£çš„å†…å®¹
        let content = r#"# Rust æ‰€æœ‰æƒç³»ç»Ÿ

Rust çš„æ‰€æœ‰æƒç³»ç»Ÿæ˜¯å…¶æœ€ç‹¬ç‰¹çš„ç‰¹æ€§ä¹‹ä¸€ã€‚

## æ ¸å¿ƒè§„åˆ™

1. æ¯ä¸ªå€¼éƒ½æœ‰ä¸€ä¸ªæ‰€æœ‰è€…
2. åŒä¸€æ—¶é—´åªèƒ½æœ‰ä¸€ä¸ªæ‰€æœ‰è€…  
3. å½“æ‰€æœ‰è€…ç¦»å¼€ä½œç”¨åŸŸï¼Œå€¼è¢«ä¸¢å¼ƒ

## ä¸ºä»€ä¹ˆé‡è¦

æ‰€æœ‰æƒè®© Rust èƒ½å¤Ÿåœ¨æ²¡æœ‰åƒåœ¾å›æ”¶å™¨çš„æƒ…å†µä¸‹ä¿è¯å†…å­˜å®‰å…¨ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚

## å®é™…åº”ç”¨

åœ¨ç³»ç»Ÿç¼–ç¨‹ã€åµŒå…¥å¼å¼€å‘ã€Web åç«¯ç­‰åœºæ™¯éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚"#;

        let args = StoreArgs {
            content: content.to_string(),
            thread_id: "".to_string(),
            metadata: None,
            auto_generate_layers: Some(true), // å¯ç”¨ LLM ç”Ÿæˆ
            scope: "user".to_string(),
            user_id: Some("llm_l0_test".to_string()),
            agent_id: None,
        };
        
        let start = std::time::Instant::now();
        let result = ops.store(args).await.unwrap();
        let duration = start.elapsed();
        
        println!("âœ… LLM L0 ç”Ÿæˆè€—æ—¶: {:?}", duration);
        println!("ğŸ“„ å­˜å‚¨ URI: {}", result.uri);
        
        // è·å– L0 æ‘˜è¦
        let l0 = ops.get_abstract(&result.uri).await.unwrap();
        println!("ğŸ“ L0 æ‘˜è¦ ({} tokens): {}", l0.token_count, l0.abstract_text);
        
        // éªŒè¯ L0 è´¨é‡ï¼ˆä½¿ç”¨å­—ç¬¦æ•°è€Œä¸æ˜¯ token æ•°ï¼Œå› ä¸ºä¸­æ–‡ token è®¡ç®—ä¸å‡†ç¡®ï¼‰
        let char_count = l0.abstract_text.chars().count();
        println!("ğŸ“ L0 å­—ç¬¦æ•°: {}", char_count);
        assert!(char_count > 20, "LLM ç”Ÿæˆçš„ L0 åº”è¯¥æœ‰å®è´¨å†…å®¹ ({} å­—ç¬¦)", char_count);
        assert!(char_count < 2000, "L0 åº”è¯¥ç›¸å¯¹ç®€æ´ ({} å­—ç¬¦)", char_count);
        
        // éªŒè¯åŒ…å«å…³é”®ä¿¡æ¯ï¼ˆLLM åº”è¯¥æå–å‡ºå…³é”®æ¦‚å¿µï¼‰
        let has_keywords = l0.abstract_text.contains("æ‰€æœ‰æƒ") || 
                          l0.abstract_text.contains("Rust") ||
                          l0.abstract_text.contains("å†…å­˜å®‰å…¨") ||
                          l0.abstract_text.contains("owner") ||
                          l0.abstract_text.contains("memory");
        assert!(has_keywords, "L0 åº”è¯¥åŒ…å«å…³é”®ä¸»é¢˜è¯: {}", l0.abstract_text);
    }

    /// æµ‹è¯• LLM ç”Ÿæˆçš„ L1 æ¦‚è§ˆ
    #[tokio::test]
    async fn test_llm_l1_quality() {
        let env = setup_llm_test_env().await;
        if env.is_none() {
            println!("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæ²¡æœ‰ LLM é…ç½®");
            return;
        }
        
        let (_temp_dir, ops) = env.unwrap();
        
        let content = r#"# OAuth 2.0 è®¤è¯æ¡†æ¶

OAuth 2.0 æ˜¯ä¸€ç§æˆæƒæ¡†æ¶ï¼Œå…è®¸ç¬¬ä¸‰æ–¹åº”ç”¨è·å–å¯¹ç”¨æˆ·èµ„æºçš„æœ‰é™è®¿é—®æƒé™ã€‚

## æˆæƒæ¨¡å¼

### 1. æˆæƒç æ¨¡å¼
æœ€å®‰å…¨ã€æœ€å¸¸ç”¨çš„æ¨¡å¼ï¼Œé€‚ç”¨äºæœ‰åç«¯çš„åº”ç”¨ã€‚

### 2. ç®€åŒ–æ¨¡å¼
é€‚ç”¨äºçº¯å‰ç«¯åº”ç”¨ã€‚

### 3. å¯†ç å‡­è¯æ¨¡å¼
ç”¨æˆ·ç›´æ¥å‘å®¢æˆ·ç«¯æä¾›ç”¨æˆ·åå¯†ç ã€‚

### 4. å®¢æˆ·ç«¯å‡­è¯æ¨¡å¼
ç”¨äºæœåŠ¡å™¨ä¹‹é—´çš„é€šä¿¡ã€‚

## å®‰å…¨è€ƒè™‘

- ä½¿ç”¨ HTTPS
- éªŒè¯ redirect_uri
- è®¾ç½®åˆç†çš„ä»¤ç‰Œè¿‡æœŸæ—¶é—´"#;

        let args = StoreArgs {
            content: content.to_string(),
            thread_id: "".to_string(),
            metadata: None,
            auto_generate_layers: Some(true),
            scope: "user".to_string(),
            user_id: Some("llm_l1_test".to_string()),
            agent_id: None,
        };
        
        let start = std::time::Instant::now();
        let result = ops.store(args).await.unwrap();
        let duration = start.elapsed();
        
        println!("âœ… LLM L1 ç”Ÿæˆè€—æ—¶: {:?}", duration);
        
        // è·å– L1 æ¦‚è§ˆ
        let l1 = ops.get_overview(&result.uri).await.unwrap();
        println!("ğŸ“ L1 æ¦‚è§ˆ ({} tokens):", l1.token_count);
        println!("{}", l1.overview_text);
        
        // éªŒè¯ L1 ç»“æ„
        assert!(l1.token_count > 50, "L1 åº”è¯¥æœ‰è¯¦ç»†å†…å®¹");
        assert!(l1.overview_text.contains("#"), "L1 åº”è¯¥åŒ…å« Markdown æ ‡é¢˜");
        
        // éªŒè¯ L1 æœ‰å®è´¨å†…å®¹ï¼ˆLLM ç”Ÿæˆçš„å¯èƒ½æ¯”åŸæ–‡é•¿ï¼Œå› ä¸ºä¼šæ‰©å±•è§£é‡Šï¼‰
        assert!(
            l1.token_count > 100,
            "L1 ({} tokens) åº”è¯¥æœ‰è¯¦ç»†å†…å®¹",
            l1.token_count
        );
    }

    /// å¯¹æ¯” Fallback å’Œ LLM ç”Ÿæˆçš„è´¨é‡å·®å¼‚
    #[tokio::test]
    async fn test_llm_vs_fallback_quality() {
        let env = setup_llm_test_env().await;
        if env.is_none() {
            println!("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæ²¡æœ‰ LLM é…ç½®");
            return;
        }
        
        let (temp_dir, ops_with_llm) = env.unwrap();
        
        // åˆ›å»ºä¸å¸¦ LLM çš„ç‰ˆæœ¬
        let ops_fallback = MemoryOperations::from_data_dir(
            temp_dir.path().to_str().unwrap()
        ).await.unwrap();
        
        let content = "Rust æ˜¯ä¸€ç§ç³»ç»Ÿç¼–ç¨‹è¯­è¨€ï¼Œä¸“æ³¨äºå®‰å…¨ã€å¹¶å‘å’Œæ€§èƒ½ã€‚å®ƒé€šè¿‡æ‰€æœ‰æƒç³»ç»Ÿåœ¨æ²¡æœ‰åƒåœ¾å›æ”¶å™¨çš„æƒ…å†µä¸‹ä¿è¯å†…å­˜å®‰å…¨ã€‚";
        
        // LLM ç‰ˆæœ¬
        let llm_args = StoreArgs {
            content: content.to_string(),
            thread_id: "".to_string(),
            metadata: None,
            auto_generate_layers: Some(true),
            scope: "user".to_string(),
            user_id: Some("llm_compare".to_string()),
            agent_id: None,
        };
        
        let llm_result = ops_with_llm.store(llm_args).await.unwrap();
        let llm_l0 = ops_with_llm.get_abstract(&llm_result.uri).await.unwrap();
        
        // Fallback ç‰ˆæœ¬
        let fallback_args = StoreArgs {
            content: content.to_string(),
            thread_id: "".to_string(),
            metadata: None,
            auto_generate_layers: Some(true),
            scope: "user".to_string(),
            user_id: Some("fallback_compare".to_string()),
            agent_id: None,
        };
        
        let fallback_result = ops_fallback.store(fallback_args).await.unwrap();
        let fallback_l0 = ops_fallback.get_abstract(&fallback_result.uri).await.unwrap();
        
        println!("ğŸ¤– LLM L0 ({} tokens): {}", llm_l0.token_count, llm_l0.abstract_text);
        println!("ğŸ“‹ Fallback L0 ({} tokens): {}", fallback_l0.token_count, fallback_l0.abstract_text);
        
        // LLM ç‰ˆæœ¬é€šå¸¸æ›´æ™ºèƒ½ï¼ˆä¸ä¸€å®šæ˜¯æ›´çŸ­ï¼Œä½†åº”è¯¥æ›´æœ‰ä¿¡æ¯é‡ï¼‰
        println!("\nğŸ“Š å¯¹æ¯”: LLM {} tokens vs Fallback {} tokens", 
            llm_l0.token_count, fallback_l0.token_count);
    }
}

mod llm_memory_extraction_tests {
    use super::*;

    /// æµ‹è¯• LLM è®°å¿†æå–ï¼ˆå¦‚æœ AutoExtractor å¯ç”¨ï¼‰
    #[tokio::test]
    async fn test_llm_memory_extraction() {
        let env = setup_llm_test_env().await;
        if env.is_none() {
            println!("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæ²¡æœ‰ LLM é…ç½®");
            return;
        }
        
        let (_temp_dir, ops) = env.unwrap();
        
        // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿå¯¹è¯
        let thread_id = "extraction_test";
        ops.add_message(thread_id, "user", "æˆ‘å–œæ¬¢ç”¨ Rust ç¼–ç¨‹ï¼Œå› ä¸ºå®ƒå†…å­˜å®‰å…¨ä¸”æ€§èƒ½é«˜ã€‚").await.unwrap();
        ops.add_message(thread_id, "assistant", "æ˜¯çš„ï¼ŒRust çš„æ‰€æœ‰æƒç³»ç»Ÿç¡®å®å¾ˆç‹¬ç‰¹ã€‚ä½ è¿˜å–œæ¬¢å…¶ä»–ä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ").await.unwrap();
        ops.add_message(thread_id, "user", "æˆ‘ä¹Ÿå–œæ¬¢ Pythonï¼Œé€‚åˆå¿«é€ŸåŸå‹å¼€å‘ã€‚").await.unwrap();
        
        // å…³é—­ session è§¦å‘æå–ï¼ˆå¦‚æœ AutoExtractor é…ç½®å¥½ï¼‰
        ops.close_session(thread_id).await.ok();
        
        println!("âœ… å¯¹è¯å·²å­˜å‚¨ï¼ŒLLM æå–åº”åœ¨åå°å®Œæˆ");
        
        // éªŒè¯ session å­˜åœ¨
        let session = ops.get_session(thread_id).await;
        assert!(session.is_ok(), "Session åº”è¯¥å­˜åœ¨");
    }
}

mod llm_performance_tests {
    use super::*;

    /// æµ‹è¯• LLM API è°ƒç”¨æ€§èƒ½
    #[tokio::test]
    async fn test_llm_api_performance() {
        let env = setup_llm_test_env().await;
        if env.is_none() {
            println!("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæ²¡æœ‰ LLM é…ç½®");
            return;
        }
        
        let (_temp_dir, ops) = env.unwrap();
        
        let content = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•å†…å®¹ï¼Œç”¨äºæµ‹é‡ LLM API è°ƒç”¨çš„æ—¶é—´ã€‚";
        
        let start = std::time::Instant::now();
        
        let args = StoreArgs {
            content: content.to_string(),
            thread_id: "".to_string(),
            metadata: None,
            auto_generate_layers: Some(true),
            scope: "user".to_string(),
            user_id: Some("perf_test".to_string()),
            agent_id: None,
        };
        
        let result = ops.store(args).await.unwrap();
        let duration = start.elapsed();
        
        println!("â±ï¸ LLM ç”Ÿæˆ L0/L1 æ€»è€—æ—¶: {:?}", duration);
        println!("ğŸ“„ URI: {}", result.uri);
        
        // é€šå¸¸ LLM è°ƒç”¨éœ€è¦ 1-5 ç§’
        assert!(duration.as_secs() < 30, "LLM ç”Ÿæˆåº”åœ¨ 30 ç§’å†…å®Œæˆ");
    }

    /// æ‰¹é‡ LLM ç”Ÿæˆæµ‹è¯•
    #[tokio::test]
    async fn test_batch_llm_generation() {
        let env = setup_llm_test_env().await;
        if env.is_none() {
            println!("âš ï¸ è·³è¿‡æµ‹è¯•ï¼šæ²¡æœ‰ LLM é…ç½®");
            return;
        }
        
        let (_temp_dir, ops) = env.unwrap();
        
        let contents = vec![
            "Rust æ‰€æœ‰æƒç³»ç»Ÿä»‹ç»...",
            "OAuth 2.0 è®¤è¯æµç¨‹è¯´æ˜...",
            "PostgreSQL æ•°æ®åº“ä¼˜åŒ–æŠ€å·§...",
        ];
        
        let start = std::time::Instant::now();
        
        for (i, content) in contents.iter().enumerate() {
            let args = StoreArgs {
                content: content.to_string(),
                thread_id: "".to_string(),
                metadata: None,
                auto_generate_layers: Some(true),
                scope: "user".to_string(),
                user_id: Some(format!("batch_user_{}", i)),
                agent_id: None,
            };
            
            let result = ops.store(args).await.unwrap();
            println!("âœ… ç¬¬ {} ä¸ªå®Œæˆ: {}", i + 1, result.uri);
        }
        
        let duration = start.elapsed();
        println!("â±ï¸ æ‰¹é‡ {} ä¸ª LLM ç”Ÿæˆæ€»è€—æ—¶: {:?}", contents.len(), duration);
        
        // æ‰¹é‡ç”Ÿæˆå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼ˆå–å†³äº API å“åº”é€Ÿåº¦ï¼‰
        assert!(duration.as_secs() < 180, "æ‰¹é‡ LLM ç”Ÿæˆåº”åœ¨ 3 åˆ†é’Ÿå†…å®Œæˆ");
    }
}

// ==================== ä½¿ç”¨è¯´æ˜ ====================
//
// è¿è¡Œè¿™äº›æµ‹è¯•éœ€è¦é…ç½® LLM APIï¼š
//
// æ–¹å¼ 1: ä½¿ç”¨ config.tomlï¼ˆæ¨èï¼‰
// ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•æœ‰ config.toml ä¸”åŒ…å«ï¼š
// [llm]
// api_base_url = "https://your-api-endpoint.com/v1"
// api_key = "your-api-key"
// model_efficient = "gpt-3.5-turbo"
//
// æ–¹å¼ 2: ä½¿ç”¨ç¯å¢ƒå˜é‡
// export LLM_API_BASE_URL="https://your-api-endpoint.com/v1"
// export LLM_API_KEY="your-api-key"
// export LLM_MODEL="gpt-3.5-turbo"
//
// ç„¶åè¿è¡Œæµ‹è¯•ï¼š
// cargo test -p cortex-mem-tools --test llm_integration_tests
//
// å¦‚æœæ²¡æœ‰é…ç½®ï¼Œæµ‹è¯•ä¼šè‡ªåŠ¨è·³è¿‡å¹¶æ˜¾ç¤ºè­¦å‘Š
